{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script generates nifti files from the parseEPI-outputted EPI time series .mat files\n",
    "(ending in \"_interp.mat\") from the listening and reading control tasks. The purpose of\n",
    "this step is to provide easy-to-use inputs for generating pymvpa datasets that can then\n",
    "be used for hyperalignment (or other pymvpa friendly things like decoding).\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% description\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% imports\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from nilearn import image as nImage\n",
    "import nibabel as nb\n",
    "from nilearn import input_data\n",
    "import scipy.io as sio\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select hyperscanning task\n",
    "# 3: listening\n",
    "# 4: reading\n",
    "task = 4\n",
    "\n",
    "# Select whether or not to generate two short nifti files for the purposes of debugging\n",
    "# hyperalignment scripts. If false, we will glob all relevant .mat files for the selected\n",
    "# hyperscanning task. If True, the selected hyperscanning task (above) is irrelevant.\n",
    "debug = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% housekeeping\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# output path and debugging output string tag\n",
    "if debug:\n",
    "    saveFolder = '/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hyperscanning/preprocessing/hyperalignment/input_nifti_files/debugging/'\n",
    "    debugStr = '_debug'\n",
    "else:\n",
    "    saveFolder = '/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hyperscanning/preprocessing/hyperalignment/input_nifti_files/'\n",
    "    debugStr = ''\n",
    "\n",
    "# set base path and search terms (nuisRegr .nii output first, parseEPI .mat output second)\n",
    "searchTerms = [[]] * 2\n",
    "basePath = '/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hyperscanning/control_tasks/nuisRegr_output_files/'\n",
    "searchTerms[0] = '*storytelling' + str(task) + '_run-0' + str(task) + '_bold_space-MNI152NLin2009cAsym_preproc_nuisRegr_newMask.nii.gz'\n",
    "searchTerms[1] = '*storytelling' + str(task) + '_run-0' + str(task) + '_bold_space-MNI152NLin2009cAsym_preproc_nuisRegr_newMask_interp.mat'\n",
    "\n",
    "# glob files\n",
    "files = [[]] * 2\n",
    "for TERM in range(len(searchTerms)):\n",
    "    files[TERM] = glob.glob(basePath + searchTerms[TERM], recursive=True)\n",
    "\n",
    "# get corresponding .nii and .mat pairs\n",
    "filePairs = []\n",
    "for NIIFILE in range(len(files[0])):\n",
    "\n",
    "    # get portion of current nifti file path that identifies participant and task\n",
    "    filePath = files[0][NIIFILE]\n",
    "    subStrStart = filePath.find('sub-')\n",
    "    subStrEnd = filePath.find('storytelling' + str(task)) + len('storytelling' + str(task))\n",
    "    subStr = filePath[subStrStart:subStrEnd]\n",
    "\n",
    "    # look for a matching string in the .mat files\n",
    "    for MATFILE in range(len(files[1])):\n",
    "\n",
    "        if subStr in files[1][MATFILE]:\n",
    "            filePairs.append([files[0][NIIFILE],files[1][MATFILE]])\n",
    "\n",
    "# truncate the pairs list to just the first two if we're debugging\n",
    "if debug:\n",
    "    filePairs = filePairs[:2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% set files to load\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resampledMaskFile = '/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hyperscanning/misc/mni_icbm152_nlin_asym_09c/mni_icbm152_t1_tal_nlin_asym_09c_mask_RESAMPLED.nii'\n",
    "maskImg = nImage.load_img(resampledMaskFile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% load whole brain mask\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for each subject with a pair of .nii and /mat files for the selected task...\n",
    "for SUB in range(len(filePairs)):\n",
    "\n",
    "    # initialize masker object from whole brain mask and nuisRegr output .nii file\n",
    "    masker = input_data.NiftiMasker(maskImg)\n",
    "    masker.fit_transform(filePairs[SUB][0])\n",
    "\n",
    "    # load parsed EPI time series\n",
    "    tmp = sio.loadmat(filePairs[SUB][1]) #load file\n",
    "    parsedEPI = tmp['tseries'] #get timeseries data\n",
    "\n",
    "    # truncate time series if this is for debugging\n",
    "    if debug:\n",
    "        parsedEPI = parsedEPI[0:40,:]\n",
    "\n",
    "    # standardize the time series\n",
    "    parsedEPI = stats.zscore(parsedEPI,axis=0)\n",
    "\n",
    "    #%% make new nifti with parsedEPI time series\n",
    "    outputFile = saveFolder + filePairs[SUB][1][subStrStart:-4] + debugStr + '.nii.gz'\n",
    "    print('saving file ' + str(SUB + 1) + ' of ' + str(len(filePairs)) + ' to: ')\n",
    "    print(outputFile)\n",
    "    cleaned_img = masker.inverse_transform(parsedEPI)\n",
    "    cleaned_img.to_filename(outputFile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "hypescancentral",
   "language": "python",
   "display_name": "hypeScanKernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}