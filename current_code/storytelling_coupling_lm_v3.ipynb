{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: Add more detailed description to the final .py file!\n",
    "\n",
    "This is a master script for running various versions of the sliding window\n",
    "coupling lag model for the hyperscanning project. There are four main \"modes\"\n",
    "in which this can be run.\n",
    "\n",
    "(0) DEBUG MODE: Run on a super truncated version of the dataset with short\n",
    "modeling windows and a small number of permutations to make sure nothing\n",
    "breaks.\n",
    "(1) FULL CONCATENATION MODE: Concatenate all speech turns into one long time\n",
    "series, run the model on this concatenated time series and ignore the option\n",
    "to implement a rolling window scheme. This is a special case of the rolling\n",
    "window approach where the window size is the total number of TRs in the\n",
    "storytelling task and the number of rolling window steps is 1.\n",
    "(2) MODEL EACH TURN MODE: Run the coupling model separately for each speech\n",
    "turn and then average parameter estimates across turns. This is a special\n",
    "case of the rolling window analysis where both the window and step size are\n",
    "equal to the number of TRs per speech turn.\n",
    "(3) CUSTOM ROLLING WINDOW MODE: manually set rolling window parameters\n",
    "\n",
    "Other parameters can be tweaked in the \"Parameter setup\" section.\n",
    "\n",
    "Note that the term \"TRs\" is used somewhat loosely below, as the time series we\n",
    "are working with were generated by interpolating across the original EPI time\n",
    "series at slightly different time points compared to the original TRs. However,\n",
    "despite the interpolation step, we maintain the total number of TRs from the\n",
    "original time series. So while the samples don't perfectly map onto the original\n",
    "TRs per se... it's close enough...\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% description\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/z/f00589z/.conda/envs/hypeScanCentral/lib/python3.9/site-packages/nilearn/datasets/__init__.py:87: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hyperscanning/support_scripts/')\n",
    "from couplingISC import *\n",
    "from CircleShift_3 import *\n",
    "from phaseScramble_2 import *\n",
    "from coupling_lm_permTest_v2 import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import random_sample\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.stats.multitest as multi\n",
    "import pickle\n",
    "from nilearn import image as nImage\n",
    "from nilearn import input_data\n",
    "from nilearn import datasets\n",
    "from nilearn import surface\n",
    "from nilearn import plotting\n",
    "%matplotlib inline\n",
    "# %load_ext filprofiler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% imports\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "startTime = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Get start time\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** INITIATING DEBUG MODE ***\n",
      "Running the coupling model for 2 20-TR windows.\n",
      "\n",
      "The inputted shuffling range of 41 TRs is greater than the width\n",
      "of the modeling window (20 TRs). This will not work. Defaulting to\n",
      "shuffling across the entire modeling window.\n"
     ]
    }
   ],
   "source": [
    "# save output?\n",
    "saveOutput = True\n",
    "\n",
    "# select participants to include\n",
    "dbicSubs = list(range(1,9)) # dbic participants to include\n",
    "cbsSubs = list(range(1,9)) # cbs participants to include\n",
    "\n",
    "# kind of hacky but predefining the total number of TRs that will be in each concatenated time series\n",
    "totTRs = 615\n",
    "\n",
    "# number of TRs per turn\n",
    "TRsPerTurn = 41\n",
    "\n",
    "# number of speech turns per participant per condition\n",
    "numTurns = round(totTRs / TRsPerTurn)\n",
    "\n",
    "# get speaker/listener TR indices\n",
    "turnTRs = [[]] * numTurns\n",
    "for TURN in range(numTurns):\n",
    "    if TURN == 0:\n",
    "        inds = np.array(list(range(TRsPerTurn)))\n",
    "    else:\n",
    "        inds = inds + TRsPerTurn\n",
    "    turnTRs[TURN] = inds\n",
    "\n",
    "# also kind of hacky but predefining the number of voxels\n",
    "totalVox = 69880\n",
    "\n",
    "# set some joblib parameters\n",
    "numJobs = 32 # number of parallel processes for joblib to use\n",
    "verbosity = 0 # joblib verbosity\n",
    "\n",
    "# set analysis parameters\n",
    "maxT = 6 # max TR lag to include in linear models\n",
    "permuts = 1000 # number of permutations to use for pair-wise permutation tests\n",
    "numChunks = 7 # number of voxel chunks\n",
    "voxelCoords = list(range(0,totalVox))\n",
    "padding = 'mean' # mean or zero\n",
    "pseudo = False # if True, compute coupling ISC for pseudopairs\n",
    "parallel = True # if True, use joblib to run ISC in parallel across permutations\n",
    "alpha = 0.05 # permutation test alpha level\n",
    "circShift = False # True = use circle shifting for TR shuffling, False = use phase scrambling\n",
    "shuffleRange = 41 # Number of TRs over which to implement shuffling. If zero, use entire time series.\n",
    "\n",
    "#################################\n",
    "### Rolling window parameters ###\n",
    "#################################\n",
    "\n",
    "# set rolling window mode to 1 of 4 options:\n",
    "# (0) DEBUG MODE:\n",
    "# (1) FULL CONCATENATION MODE: Concatenate all speech turns into one long time\n",
    "# series, run the model on this concatenated time series and ignore the option\n",
    "# to implement a rolling window scheme. This is a special case of the rolling\n",
    "# window approach where the window size is the total number of TRs in the\n",
    "# storytelling task and the number of rolling window steps is 1.\n",
    "# (2) MODEL EACH TURN MODE: Run the coupling model separately for each speech\n",
    "# turn and then average parameter estimates across turns. This is a special\n",
    "# case of the rolling window analysis where both the window and step size are\n",
    "# equal to the number of TRs per speech turn.\n",
    "# (3) CUSTOM ROLLING WINDOW MODE: manually set rolling window parameters\n",
    "rollMode = 0\n",
    "\n",
    "# Set rolling window parameters based on MODE selected above\n",
    "if rollMode == 3: # CUSTOM ROLLING WINDOW MODE\n",
    "\n",
    "    # set custom parameters\n",
    "    winTRs = 308 # window size [TRs]\n",
    "    stepSize = 307 # step size [TRs]\n",
    "    numSteps = 1 # number of steps to use if maxCoverage is set to False below [integer]\n",
    "\n",
    "    # set whether or not to let the selected window and step sizes cover the\n",
    "    # maximum number of available TRs. Note that if maxCoverage == True, numSteps\n",
    "    # will be overridden and set to the maximum number of steps determined below.\n",
    "    maxCoverage = True\n",
    "\n",
    "    # compute maximum possible number of steps based on winTR and stepSize\n",
    "    maxSteps = int(math.ceil((totTRs - winTRs + 1) / stepSize))\n",
    "    if maxCoverage: # if max coverage selected...\n",
    "        numSteps = maxSteps # redefine numSteps to maximum possible number of steps\n",
    "\n",
    "    # find any TRs that will be missed given winTRs, stepSize, and maxSteps\n",
    "    lastTr = winTRs + (numSteps - 1) * stepSize # last TR this approach will analyze\n",
    "    TRsLeftOut = totTRs - lastTr # number of TRs that will be left out by the current approach\n",
    "\n",
    "    # feedback\n",
    "    print('\\n*** INITIATING CUSTOM ROLLING WINDOW MODE ***')\n",
    "    print('Rolling window size: ' + str(winTRs) + ' TRs')\n",
    "    print('Step size: ' + str(stepSize) + ' TRs')\n",
    "    print('Number of steps: ' + str(numSteps))\n",
    "\n",
    "    if TRsLeftOut > 0: # if any TRs will end up getting left out based on window and step sizes...\n",
    "        print('WARNING! The last ' + str(TRsLeftOut) + ' TRs of the storytelling task will be left out due to the selected window and step sizes!')\n",
    "    elif TRsLeftOut <= 0:\n",
    "        print('WARNING! The inputted rolling window parameters assume a larger time series than we have! Something needs to change!')\n",
    "\n",
    "elif rollMode == 1: # FULL CONCATENATION MODE\n",
    "\n",
    "    # set parameters\n",
    "    winTRs = totTRs # use all TRs\n",
    "    stepSize = np.nan # no step size as there is only the one window\n",
    "    numSteps = 1 # just one step\n",
    "\n",
    "    # feedback\n",
    "    print('\\n*** INITIATING FULL CONCATENATION MODE ***')\n",
    "    print('Running coupling model on single time series concatenated from')\n",
    "    print('all speech turns of a given speaker-listener orientation.')\n",
    "\n",
    "elif rollMode == 2: # MODEL EACH TURN MODE\n",
    "\n",
    "    # set parameters\n",
    "    winTRs = 41 # use all TRs\n",
    "    stepSize = 41 # no step size as there is only the one window\n",
    "    numSteps = 15 # 615 total TRs / 41 TRs per speech turn = 15 steps\n",
    "\n",
    "    # feedback\n",
    "    print('\\n*** INITIATING MODEL EACH TURN MODE ***')\n",
    "    print('Running the coupling model separately for each speech turn and then')\n",
    "    print('averaging parameter estimates across turns.')\n",
    "\n",
    "elif rollMode == 0: # DEBUG MODE\n",
    "\n",
    "    # set parameters so we just look at two super short windows\n",
    "    winTRs = 20 # window size [TRs]\n",
    "    stepSize = 20 # step size [TRs]\n",
    "    numSteps = 2 # number of steps to use if maxCoverage is set to False below [integer]\n",
    "\n",
    "    # feedback\n",
    "    print('\\n*** INITIATING DEBUG MODE ***')\n",
    "    print('Running the coupling model for ' + str(numSteps) + ' ' + str(winTRs) + '-TR windows.')\n",
    "\n",
    "    # reset some additional parameters to make things quick\n",
    "    permuts = 10\n",
    "    numChunks = 2\n",
    "    maxT = 2\n",
    "\n",
    "# get TRs to analyze in each step of rolling window analysis\n",
    "stepTRs = [[]] * numSteps\n",
    "for STEP in range(numSteps):\n",
    "    if STEP == 0:\n",
    "        stepTRs[STEP] = np.array(list(range(winTRs)))\n",
    "    else:\n",
    "        stepTRs[STEP] = stepTRs[STEP-1] + stepSize\n",
    "\n",
    "# check the shuffleRange variable, reset if necessary, and provide some feedback\n",
    "if shuffleRange > winTRs:\n",
    "    print('\\nThe inputted shuffling range of '+  str(shuffleRange)  + ' TRs is greater than the width')\n",
    "    print('of the modeling window (' + str(winTRs) + ' TRs). This will not work. Defaulting to')\n",
    "    print('shuffling across the entire modeling window.')\n",
    "    shuffleRange = 0 # reset shuffle range to default of entire rolling window\n",
    "else:\n",
    "    if shuffleRange == 0 or shuffleRange == winTRs:\n",
    "        print('\\nShuffling across the entire modeling window of ' + str(winTRs) + ' TRs.')\n",
    "    elif 0 < shuffleRange < winTRs:\n",
    "        print('\\nShuffling across ' + str(shuffleRange) + '-TR subsets of the modeling window of ' + str(winTRs) + ' TRs.')\n",
    "if shuffleRange == TRsPerTurn:\n",
    "    print('\\nNote that this means we are shuffling at the level of speech turns.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Parameter setup\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pair       DBIC        CBS  ind_run  joint_run\n",
      "0     1  sid000014  hid000001        2          1\n",
      "1     2  sid000007  hid000002        2          1\n",
      "2     3  sid000009  hid000003        2          1\n",
      "3     4  sid000560  hid000004        1          2\n",
      "4     5  sid000535  hid000005        1          2\n",
      "5     6  sid000102  hid000006        2          1\n",
      "6     7  sid000416  hid000007        2          1\n",
      "7     8  sid000499  hid000008        1          2\n",
      "8     9  sid000142  hid000009        1          2\n"
     ]
    }
   ],
   "source": [
    "# seed the RNG\n",
    "rngSeed = 8675309\n",
    "rng = np.random.default_rng(rngSeed)\n",
    "\n",
    "# mark starting time\n",
    "startTime = time.time()\n",
    "\n",
    "# load pair and run data\n",
    "pairsAndRuns = pd.read_csv(r'/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hyperscanning/misc/hyperscanning_pair_and_run_lookup.csv')\n",
    "print(pairsAndRuns)\n",
    "\n",
    "# set data folder\n",
    "dataFolder = '/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hyperscanning/storytelling/parseEPI_output_files/'\n",
    "\n",
    "# set output folder\n",
    "outputFolder = '/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hyperscanning/storytelling/lag_model_output/'\n",
    "\n",
    "# get numbers of pairs\n",
    "realPairN = len(list(set(dbicSubs) & set(cbsSubs)))\n",
    "pseudoPairN = len(dbicSubs)*len(cbsSubs) - realPairN\n",
    "totalPairN = realPairN + pseudoPairN\n",
    "\n",
    "# set column names for 'pairMap' dataframe to be used as a reference for pair information\n",
    "pairMap_header = ['dbicNum', #DBIC subject number\n",
    "                  'dbicID', #DBIC subject ID (e.g., sid000007)\n",
    "                  'cbsNum', #CBS subject number\n",
    "                  'cbsID', #CBS subject ID\n",
    "                  'pairType', #1=real, 0=pseudo\n",
    "                  'condition', #1=joint, 0=independent\n",
    "                  'dbicSpeaker', #1=DBIC speaker, CBS listener, 0=CBS speaker, DBIC listener\n",
    "                  'sFile', #speaker file name\n",
    "                  'lFile', #listener file name\n",
    "                  'duration'] #ISC duration [min]\n",
    "\n",
    "# number of rows to preallocate for pairMap dataframe\n",
    "numRows = (realPairN + pseudoPairN) * 4\n",
    "\n",
    "# preallocate pair map data frame\n",
    "pairMap = pd.DataFrame(columns = pairMap_header, index=range(numRows))\n",
    "\n",
    "# set order in which to load tasks for DBIC and CBS subs within each pair\n",
    "dTask = ['listener','speaker','listener','speaker']\n",
    "cTask = ['speaker','listener','speaker','listener']\n",
    "\n",
    "# condition strings for feedback at top of ROW loop below\n",
    "storyConds = ['independent','joint','joint minus independent']\n",
    "pairTypes = ['pseudo','real']\n",
    "speakers = ['CBS','DBIC']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% other housekeeping\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# initialize pairMap row counter\n",
    "ROW = 0\n",
    "\n",
    "# for each DBIC subject...\n",
    "for dbicPairN in dbicSubs:\n",
    "\n",
    "    # get DBIC sub ID and run info\n",
    "    dbicSub = pairsAndRuns['DBIC'][dbicPairN] # e.g., 'sid000007'\n",
    "    dbicRunInd = pairsAndRuns['ind_run'][dbicPairN] # current subject's independent run #\n",
    "    dbicRunJoint = pairsAndRuns['joint_run'][dbicPairN] # current subject's joint run #\n",
    "    dRun = [dbicRunInd, dbicRunInd, dbicRunJoint, dbicRunJoint] # make an array of independent and joint run #s to simplify loading below\n",
    "\n",
    "    # get dbic file names\n",
    "    dbicFiles = [[]]*4\n",
    "    for FILE in range(4):\n",
    "        dbicFiles[FILE] = dataFolder + 'sub-' + dbicSub + '_ses-pair0' + str(dbicPairN + 1) + '_task-storytelling' + \\\n",
    "            str(dRun[FILE]) + '_run-0' + str(dRun[FILE]) + '_bold_space-MNI152NLin2009cAsym_preproc_nuisRegr_2021_' + dTask[FILE] + '.mat'  # [TRs x voxels]\n",
    "\n",
    "    # for each CBS subject...\n",
    "    for cbsPairN in cbsSubs:\n",
    "\n",
    "        # get CBS sub ID and run info\n",
    "        cbsSub = pairsAndRuns['CBS'][cbsPairN] # e.g., 'hid000002'\n",
    "        cbsRunInd = pairsAndRuns['ind_run'][cbsPairN]\n",
    "        cbsRunJoint = pairsAndRuns['joint_run'][cbsPairN]\n",
    "        cRun = [cbsRunInd, cbsRunInd, cbsRunJoint, cbsRunJoint] # make an array of independent and joint run #s to simplify loading below\n",
    "\n",
    "        for FILE in range(4):\n",
    "\n",
    "            # get dbic file names\n",
    "            cbsFiles = [[]] * 4\n",
    "            cbsFiles[FILE] = dataFolder + 'sub-' + cbsSub + '_ses-pair0' + str(cbsPairN + 1) + '_task-storytelling' + \\\n",
    "                             str(cRun[FILE]) + '_run-0' + str(cRun[FILE]) + \\\n",
    "                             '_bold_space-MNI152NLin2009cAsym_preproc_nuisRegr_2021_' + cTask[FILE] + '.mat'  # [TRs x voxels]\n",
    "\n",
    "            ##############################\n",
    "            ### index stuff to pairMap ###\n",
    "            ##############################\n",
    "\n",
    "            # subject numbers and IDs\n",
    "            pairMap['dbicNum'][ROW] = dbicPairN + 1\n",
    "            pairMap['dbicID'][ROW] = dbicSub\n",
    "            pairMap['cbsNum'][ROW] = cbsPairN + 1\n",
    "            pairMap['cbsID'][ROW] = cbsSub\n",
    "\n",
    "            # pair type\n",
    "            if pairMap['dbicNum'][ROW] == pairMap['cbsNum'][ROW]:\n",
    "                pairMap['pairType'][ROW] = 1 #real pair\n",
    "            else:\n",
    "                pairMap['pairType'][ROW] = 0 #pseudo pair\n",
    "\n",
    "            # speaker/listener\n",
    "            if ROW % 2: # if it's an odd row - DBIC speaker, CBS listener\n",
    "                pairMap['dbicSpeaker'][ROW] = 1\n",
    "                pairMap['sFile'][ROW] = dbicFiles[FILE] # speaker file name\n",
    "                pairMap['lFile'][ROW] = cbsFiles[FILE] # listener file name\n",
    "            else: # if it's an even row - CBS speaker, DBIC listener\n",
    "                pairMap['dbicSpeaker'][ROW] = 0\n",
    "                pairMap['sFile'][ROW] = cbsFiles[FILE] # speaker file name\n",
    "                pairMap['lFile'][ROW] = dbicFiles[FILE] # listener file name\n",
    "\n",
    "            # independent/joint condition\n",
    "            if FILE < 2:\n",
    "                pairMap['condition'][ROW] = 0 # independent\n",
    "            else:\n",
    "                pairMap['condition'][ROW] = 1 # joint\n",
    "\n",
    "            ROW += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% fill in pairMap dataframe\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   dbicNum     dbicID cbsNum      cbsID pairType condition dbicSpeaker  \\\n0        2  sid000007      2  hid000002        1         0           0   \n1        2  sid000007      2  hid000002        1         0           1   \n2        2  sid000007      2  hid000002        1         1           0   \n3        2  sid000007      2  hid000002        1         1           1   \n4        3  sid000009      3  hid000003        1         0           0   \n5        3  sid000009      3  hid000003        1         0           1   \n6        3  sid000009      3  hid000003        1         1           0   \n7        3  sid000009      3  hid000003        1         1           1   \n8        4  sid000560      4  hid000004        1         0           0   \n9        4  sid000560      4  hid000004        1         0           1   \n10       4  sid000560      4  hid000004        1         1           0   \n11       4  sid000560      4  hid000004        1         1           1   \n12       5  sid000535      5  hid000005        1         0           0   \n13       5  sid000535      5  hid000005        1         0           1   \n14       5  sid000535      5  hid000005        1         1           0   \n15       5  sid000535      5  hid000005        1         1           1   \n16       6  sid000102      6  hid000006        1         0           0   \n17       6  sid000102      6  hid000006        1         0           1   \n18       6  sid000102      6  hid000006        1         1           0   \n19       6  sid000102      6  hid000006        1         1           1   \n20       7  sid000416      7  hid000007        1         0           0   \n21       7  sid000416      7  hid000007        1         0           1   \n22       7  sid000416      7  hid000007        1         1           0   \n23       7  sid000416      7  hid000007        1         1           1   \n24       8  sid000499      8  hid000008        1         0           0   \n25       8  sid000499      8  hid000008        1         0           1   \n26       8  sid000499      8  hid000008        1         1           0   \n27       8  sid000499      8  hid000008        1         1           1   \n28       9  sid000142      9  hid000009        1         0           0   \n29       9  sid000142      9  hid000009        1         0           1   \n30       9  sid000142      9  hid000009        1         1           0   \n31       9  sid000142      9  hid000009        1         1           1   \n\n                                                sFile  \\\n0   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n1   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n2   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n3   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n4   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n5   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n6   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n7   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n8   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n9   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n10  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n11  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n12  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n13  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n14  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n15  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n16  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n17  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n18  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n19  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n20  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n21  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n22  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n23  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n24  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n25  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n26  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n27  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n28  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n29  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n30  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n31  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n\n                                                lFile duration  \n0   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n1   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n2   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n3   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n4   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n5   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n6   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n7   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n8   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n9   /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n10  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n11  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n12  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n13  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n14  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n15  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n16  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n17  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n18  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n19  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n20  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n21  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n22  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n23  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n24  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n25  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n26  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n27  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n28  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n29  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n30  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n31  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dbicNum</th>\n      <th>dbicID</th>\n      <th>cbsNum</th>\n      <th>cbsID</th>\n      <th>pairType</th>\n      <th>condition</th>\n      <th>dbicSpeaker</th>\n      <th>sFile</th>\n      <th>lFile</th>\n      <th>duration</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>sid000007</td>\n      <td>2</td>\n      <td>hid000002</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>sid000007</td>\n      <td>2</td>\n      <td>hid000002</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>sid000007</td>\n      <td>2</td>\n      <td>hid000002</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>sid000007</td>\n      <td>2</td>\n      <td>hid000002</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>sid000009</td>\n      <td>3</td>\n      <td>hid000003</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>sid000009</td>\n      <td>3</td>\n      <td>hid000003</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3</td>\n      <td>sid000009</td>\n      <td>3</td>\n      <td>hid000003</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>sid000009</td>\n      <td>3</td>\n      <td>hid000003</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>4</td>\n      <td>sid000560</td>\n      <td>4</td>\n      <td>hid000004</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>4</td>\n      <td>sid000560</td>\n      <td>4</td>\n      <td>hid000004</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>4</td>\n      <td>sid000560</td>\n      <td>4</td>\n      <td>hid000004</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>4</td>\n      <td>sid000560</td>\n      <td>4</td>\n      <td>hid000004</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>5</td>\n      <td>sid000535</td>\n      <td>5</td>\n      <td>hid000005</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>5</td>\n      <td>sid000535</td>\n      <td>5</td>\n      <td>hid000005</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>5</td>\n      <td>sid000535</td>\n      <td>5</td>\n      <td>hid000005</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>5</td>\n      <td>sid000535</td>\n      <td>5</td>\n      <td>hid000005</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>6</td>\n      <td>sid000102</td>\n      <td>6</td>\n      <td>hid000006</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>6</td>\n      <td>sid000102</td>\n      <td>6</td>\n      <td>hid000006</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>6</td>\n      <td>sid000102</td>\n      <td>6</td>\n      <td>hid000006</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>6</td>\n      <td>sid000102</td>\n      <td>6</td>\n      <td>hid000006</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>7</td>\n      <td>sid000416</td>\n      <td>7</td>\n      <td>hid000007</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>7</td>\n      <td>sid000416</td>\n      <td>7</td>\n      <td>hid000007</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>7</td>\n      <td>sid000416</td>\n      <td>7</td>\n      <td>hid000007</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>7</td>\n      <td>sid000416</td>\n      <td>7</td>\n      <td>hid000007</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>8</td>\n      <td>sid000499</td>\n      <td>8</td>\n      <td>hid000008</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>8</td>\n      <td>sid000499</td>\n      <td>8</td>\n      <td>hid000008</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>8</td>\n      <td>sid000499</td>\n      <td>8</td>\n      <td>hid000008</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>8</td>\n      <td>sid000499</td>\n      <td>8</td>\n      <td>hid000008</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>9</td>\n      <td>sid000142</td>\n      <td>9</td>\n      <td>hid000009</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>9</td>\n      <td>sid000142</td>\n      <td>9</td>\n      <td>hid000009</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>9</td>\n      <td>sid000142</td>\n      <td>9</td>\n      <td>hid000009</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>9</td>\n      <td>sid000142</td>\n      <td>9</td>\n      <td>hid000009</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not pseudo: # if we are not including pseudopairs...\n",
    "    pairMap = pairMap.loc[pairMap['pairType'] == 1,:] # remove pseudopairs\n",
    "    pairMap = pairMap.reset_index(drop=True)\n",
    "    pseudoStr = 'excluding' # set string to use below\n",
    "else:\n",
    "    pseudoStr = 'including' # set string to use below\n",
    "\n",
    "# print final pairMap dataframe\n",
    "pairMap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Conditionally remove pseudopairs\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a truncated version of pairMap with only two pairs because we are in DEBUG MODE.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  dbicNum     dbicID cbsNum      cbsID pairType condition dbicSpeaker  \\\n0       2  sid000007      2  hid000002        1         0           0   \n1       2  sid000007      2  hid000002        1         0           1   \n2       2  sid000007      2  hid000002        1         1           0   \n3       2  sid000007      2  hid000002        1         1           1   \n4       3  sid000009      3  hid000003        1         0           0   \n5       3  sid000009      3  hid000003        1         0           1   \n6       3  sid000009      3  hid000003        1         1           0   \n7       3  sid000009      3  hid000003        1         1           1   \n\n                                               sFile  \\\n0  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n1  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n2  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n3  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n4  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n5  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n6  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n7  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...   \n\n                                               lFile duration  \n0  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n1  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n2  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n3  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n4  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n5  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n6  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  \n7  /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...      NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dbicNum</th>\n      <th>dbicID</th>\n      <th>cbsNum</th>\n      <th>cbsID</th>\n      <th>pairType</th>\n      <th>condition</th>\n      <th>dbicSpeaker</th>\n      <th>sFile</th>\n      <th>lFile</th>\n      <th>duration</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>sid000007</td>\n      <td>2</td>\n      <td>hid000002</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>sid000007</td>\n      <td>2</td>\n      <td>hid000002</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>sid000007</td>\n      <td>2</td>\n      <td>hid000002</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>sid000007</td>\n      <td>2</td>\n      <td>hid000002</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>sid000009</td>\n      <td>3</td>\n      <td>hid000003</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>sid000009</td>\n      <td>3</td>\n      <td>hid000003</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3</td>\n      <td>sid000009</td>\n      <td>3</td>\n      <td>hid000003</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>sid000009</td>\n      <td>3</td>\n      <td>hid000003</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>/dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hypersc...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if rollMode == 0:\n",
    "    if pairMap.shape[0] > 8: # if there are more than 8 rows in pairMap (i.e., two pairs)\n",
    "        pairMap = pairMap.iloc[range(8),:] # limit pairMap to just the first two pairs\n",
    "print('Using a truncated version of pairMap with only two pairs because we are in DEBUG MODE.')\n",
    "pairMap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Conditionally remove pairs if we're in DEBUG MODE\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "      dbicID      cbsID\n0  sid000007  hid000002\n1  sid000009  hid000003",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dbicID</th>\n      <th>cbsID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sid000007</td>\n      <td>hid000002</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sid000009</td>\n      <td>hid000003</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique pairs\n",
    "uniquePairs = pairMap[[\"dbicID\", \"cbsID\"]].drop_duplicates()\n",
    "uniquePairs = uniquePairs.reset_index(drop=True)\n",
    "\n",
    "# get independent vs joint row indices for each pair\n",
    "pairInds = [[]] * uniquePairs.shape[0]\n",
    "for PAIR in range(len(pairInds)):\n",
    "\n",
    "    pairInds[PAIR] = [[]] * 2\n",
    "    for COND in [0,1]:\n",
    "        pairInds[PAIR][COND] = pairMap[(pairMap['dbicID'] == uniquePairs['dbicID'][PAIR]) & (pairMap['cbsID'] == uniquePairs['cbsID'][PAIR]) & (pairMap['condition'] == COND)].index.tolist()\n",
    "\n",
    "# print list of unique pairs\n",
    "uniquePairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% get indices for making joint vs ind comparisons later\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# preallocate chunk indices list\n",
    "chunkInds = [[]] * numChunks\n",
    "\n",
    "# get main chunk size (if the number of chunks doesn't go into totalVox evenly there will be one remainder chunk of smaller size)\n",
    "chunkSize = np.ceil(totalVox / numChunks)\n",
    "\n",
    "# get chunk indices\n",
    "for CHUNK in range(numChunks):\n",
    "    if CHUNK == 0: # if it's the first chunk...\n",
    "\n",
    "        # get initial indices from 0 to the main chunk size\n",
    "        chunkInds[CHUNK] = np.arange(chunkSize).astype(int)\n",
    "\n",
    "    elif CHUNK == numChunks and not totalVox / numChunks == 0: # if it's the last chunk and it's a remainder chunk\n",
    "\n",
    "        # get remainder chunk indices\n",
    "        remainder = totalVox - chunkSize * (numChunks-1)\n",
    "        chunkInds[CHUNK] = (chunkInds[CHUNK-1] + chunkSize)[range(remainder)].astype(int)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # add chunk size to previous chunk indices\n",
    "        chunkInds[CHUNK] = (chunkInds[CHUNK-1] + chunkSize).astype(int)\n",
    "\n",
    "# preallocate voxel chunk array for duration\n",
    "chunkDur = [[]] * numChunks\n",
    "pairData_by_step = [[]] * numChunks\n",
    "pairData_across_steps = [[]] * numChunks\n",
    "pairData_jVSi_by_step = [[]] * numChunks\n",
    "pairData_jVSi_across_steps = [[]] * numChunks\n",
    "groupData_by_step = [[]] * numChunks\n",
    "groupData_across_steps = [[]] * numChunks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% set voxel chunking scheme\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running lag models with 2 pairs, excluding pseudo pairs\n",
      "Number of voxel chunks: 2\n",
      "Permutations: 10\n",
      "Max TR lag: 2\n",
      "\n",
      "*** CHUNK 1 OF 2: VOXELS 1 to 34940 ***\n",
      "\n",
      "estimating lag model C1R1 of C2R8\n",
      "DBIC participant: sid000007\n",
      "CBS participant: hid000002\n",
      "storytelling condition: independent\n",
      "pair type: real\n",
      "speaker: CBS\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 23.29%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 22.01%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 22.66%\n",
      "processing time for model 1: ~0.91 min\n",
      "\n",
      "estimating lag model C1R2 of C2R8\n",
      "DBIC participant: sid000007\n",
      "CBS participant: hid000002\n",
      "storytelling condition: independent\n",
      "pair type: real\n",
      "speaker: DBIC\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 18.34%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 17.93%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 18.27%\n",
      "processing time for model 2: ~0.15 min\n",
      "\n",
      "estimating lag model C1R3 of C2R8\n",
      "DBIC participant: sid000007\n",
      "CBS participant: hid000002\n",
      "storytelling condition: joint\n",
      "pair type: real\n",
      "speaker: CBS\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 21.53%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 21.12%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 20.17%\n",
      "processing time for model 3: ~0.15 min\n",
      "\n",
      "estimating lag model C1R4 of C2R8\n",
      "DBIC participant: sid000007\n",
      "CBS participant: hid000002\n",
      "storytelling condition: joint\n",
      "pair type: real\n",
      "speaker: DBIC\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 17.43%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 21.33%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 20.14%\n",
      "processing time for model 4: ~0.14 min\n",
      "\n",
      "estimating lag model C1R5 of C2R8\n",
      "DBIC participant: sid000009\n",
      "CBS participant: hid000003\n",
      "storytelling condition: independent\n",
      "pair type: real\n",
      "speaker: CBS\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 15.94%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 14.74%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 14.94%\n",
      "processing time for model 5: ~0.14 min\n",
      "\n",
      "estimating lag model C1R6 of C2R8\n",
      "DBIC participant: sid000009\n",
      "CBS participant: hid000003\n",
      "storytelling condition: independent\n",
      "pair type: real\n",
      "speaker: DBIC\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 19.07%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 15.82%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 17.84%\n",
      "processing time for model 6: ~0.14 min\n",
      "\n",
      "estimating lag model C1R7 of C2R8\n",
      "DBIC participant: sid000009\n",
      "CBS participant: hid000003\n",
      "storytelling condition: joint\n",
      "pair type: real\n",
      "speaker: CBS\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 15.7%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 20.53%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 18.16%\n",
      "processing time for model 7: ~0.14 min\n",
      "\n",
      "estimating lag model C1R8 of C2R8\n",
      "DBIC participant: sid000009\n",
      "CBS participant: hid000003\n",
      "storytelling condition: joint\n",
      "pair type: real\n",
      "speaker: DBIC\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 19.83%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 18.63%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 19.16%\n",
      "processing time for model 8: ~0.14 min\n",
      "\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 1 of the independent condition: 8.24%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 1 of the independent condition: 11.52%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 1 of the independent condition: 7.89%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 1 of the independent condition: 11.17%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the independent condition: 8.35%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the independent condition: 9.5%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the independent condition: 8.34%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the independent condition: 9.32%\n",
      "\n",
      "% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the independent condition: 7.74%\n",
      "\n",
      "% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the independent condition: 10.77%\n",
      "\n",
      "% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the independent condition: 7.62%\n",
      "\n",
      "% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the independent condition: 11.1%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 1 of the joint condition: 7.5%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 1 of the joint condition: 10.89%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 1 of the joint condition: 7.17%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 1 of the joint condition: 11.57%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint condition: 11.99%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint condition: 7.91%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint condition: 12.07%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint condition: 8.44%\n",
      "\n",
      "% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint condition: 9.15%\n",
      "\n",
      "% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint condition: 9.81%\n",
      "\n",
      "% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint condition: 8.91%\n",
      "\n",
      "% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint condition: 10.52%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 1 of the joint minus independent condition: 9.18%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 1 of the joint minus independent condition: 9.26%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 1 of the joint minus independent condition: 9.18%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 1 of the joint minus independent condition: 9.26%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 10.78%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 7.58%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 10.78%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 7.58%\n",
      "\n",
      "% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 15.13%\n",
      "\n",
      "% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 13.0%\n",
      "\n",
      "% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 15.13%\n",
      "\n",
      "% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 13.0%\n",
      "\n",
      "*** CHUNK 2 OF 2: VOXELS 34941 to 69880 ***\n",
      "\n",
      "estimating lag model C2R1 of C2R8\n",
      "DBIC participant: sid000007\n",
      "CBS participant: hid000002\n",
      "storytelling condition: independent\n",
      "pair type: real\n",
      "speaker: CBS\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 21.91%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 18.3%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 20.67%\n",
      "processing time for model 1: ~0.14 min\n",
      "\n",
      "estimating lag model C2R2 of C2R8\n",
      "DBIC participant: sid000007\n",
      "CBS participant: hid000002\n",
      "storytelling condition: independent\n",
      "pair type: real\n",
      "speaker: DBIC\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 11.88%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 17.77%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 14.19%\n",
      "processing time for model 2: ~0.14 min\n",
      "\n",
      "estimating lag model C2R3 of C2R8\n",
      "DBIC participant: sid000007\n",
      "CBS participant: hid000002\n",
      "storytelling condition: joint\n",
      "pair type: real\n",
      "speaker: CBS\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 20.11%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 20.37%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 20.31%\n",
      "processing time for model 3: ~0.13 min\n",
      "\n",
      "estimating lag model C2R4 of C2R8\n",
      "DBIC participant: sid000007\n",
      "CBS participant: hid000002\n",
      "storytelling condition: joint\n",
      "pair type: real\n",
      "speaker: DBIC\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 15.72%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 18.71%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 15.98%\n",
      "processing time for model 4: ~0.13 min\n",
      "\n",
      "estimating lag model C2R5 of C2R8\n",
      "DBIC participant: sid000009\n",
      "CBS participant: hid000003\n",
      "storytelling condition: independent\n",
      "pair type: real\n",
      "speaker: CBS\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 14.34%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 18.67%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 16.48%\n",
      "processing time for model 5: ~0.14 min\n",
      "\n",
      "estimating lag model C2R6 of C2R8\n",
      "DBIC participant: sid000009\n",
      "CBS participant: hid000003\n",
      "storytelling condition: independent\n",
      "pair type: real\n",
      "speaker: DBIC\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 16.79%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 23.11%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 18.99%\n",
      "processing time for model 6: ~0.13 min\n",
      "\n",
      "estimating lag model C2R7 of C2R8\n",
      "DBIC participant: sid000009\n",
      "CBS participant: hid000003\n",
      "storytelling condition: joint\n",
      "pair type: real\n",
      "speaker: CBS\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 17.02%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 19.57%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 17.6%\n",
      "processing time for model 7: ~0.14 min\n",
      "\n",
      "estimating lag model C2R8 of C2R8\n",
      "DBIC participant: sid000009\n",
      "CBS participant: hid000003\n",
      "storytelling condition: joint\n",
      "pair type: real\n",
      "speaker: DBIC\n",
      "\n",
      "Step 1 of 2: TRs 0 to 19\n",
      "% of voxels with signficant FDR-corrected R^2 values: 16.52%\n",
      "Step 2 of 2: TRs 20 to 39\n",
      "% of voxels with signficant FDR-corrected R^2 values: 18.2%\n",
      "\n",
      "Across steps:\n",
      "% of voxels with signficant FDR-corrected mean R^2 values: 17.07%\n",
      "processing time for model 8: ~0.14 min\n",
      "\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 1 of the independent condition: 7.21%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 1 of the independent condition: 8.57%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 1 of the independent condition: 7.14%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 1 of the independent condition: 7.84%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the independent condition: 9.27%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the independent condition: 10.4%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the independent condition: 9.25%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the independent condition: 10.82%\n",
      "\n",
      "% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the independent condition: 7.97%\n",
      "\n",
      "% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the independent condition: 9.28%\n",
      "\n",
      "% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the independent condition: 7.95%\n",
      "\n",
      "% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the independent condition: 8.99%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 1 of the joint condition: 7.4%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 1 of the joint condition: 10.22%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 1 of the joint condition: 6.7%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 1 of the joint condition: 10.42%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint condition: 11.08%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint condition: 7.28%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint condition: 11.2%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint condition: 7.54%\n",
      "\n",
      "% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint condition: 8.77%\n",
      "\n",
      "% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint condition: 8.73%\n",
      "\n",
      "% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint condition: 8.67%\n",
      "\n",
      "% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint condition: 9.18%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 1 of the joint minus independent condition: 6.76%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 1 of the joint minus independent condition: 9.5%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 1 of the joint minus independent condition: 6.76%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 1 of the joint minus independent condition: 9.5%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 11.72%\n",
      "\n",
      "% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 7.91%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 11.72%\n",
      "\n",
      "% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 7.91%\n",
      "\n",
      "% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 16.06%\n",
      "\n",
      "% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 15.89%\n",
      "\n",
      "% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 16.06%\n",
      "\n",
      "% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom 2.5% of null distribution\n",
      "in step 2 of the joint minus independent condition: 15.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/z/f00589z/.conda/envs/hypeScanCentral/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3437: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# %%filprofile\n",
    "\n",
    "# initial feedback\n",
    "print('\\nRunning lag models with ' + str(len(np.unique(pairMap['dbicNum']))) + ' pairs, ' + pseudoStr + ' pseudo pairs')\n",
    "print('Number of voxel chunks: ' + str(numChunks))\n",
    "print('Permutations: ' + str(permuts))\n",
    "print('Max TR lag: ' + str(maxT))\n",
    "\n",
    "# FOR EACH CHUNK OF VOXELS\n",
    "for CHUNK in range(numChunks):\n",
    "\n",
    "    # voxel chunk feedback\n",
    "    print('\\n*** CHUNK ' + str(CHUNK+1) + ' OF ' + str(numChunks) + ': VOXELS ' + str(np.min(chunkInds[CHUNK])+1) + ' to ' + str(np.max(chunkInds[CHUNK])+1) + ' ***\\n')\n",
    "\n",
    "    ######################\n",
    "    ### Coupling model ###\n",
    "    ######################\n",
    "\n",
    "    # initalize lists with length equal to the total number of combinations\n",
    "    # of pair, speaker-listener relationship, and joint vs independent condition\n",
    "    # (i.e., the number of rows in pairMap)\n",
    "    iscData = [[]] * pairMap.shape[0]\n",
    "    perms = [[]] * pairMap.shape[0]\n",
    "\n",
    "    # if we're using more than one modeling window, preallocate lists for cross-window parameter averages\n",
    "    if numSteps > 1:\n",
    "        paramAvg = [[]] * pairMap.shape[0]\n",
    "        perms_paramAvg = [[]] * pairMap.shape[0]\n",
    "    else:\n",
    "        paramAvg = np.nan\n",
    "        perms_paramAvg = np.nan\n",
    "\n",
    "    # for each combination of pair, speaker-listener relationship, and joint vs independent condition...\n",
    "    for ROW in range(pairMap.shape[0]):\n",
    "\n",
    "        # get current time so we can keep track of the time it takes to analyze pair data\n",
    "        # from each row of pairMap\n",
    "        rowStart = time.time()\n",
    "\n",
    "        # initialize iscData subArray for each window in the series of rolling ISC windows...\n",
    "        iscData[ROW] = [[]] * numSteps\n",
    "        perms[ROW] = [[]] * numSteps\n",
    "\n",
    "        # pair/condition-wise feedback\n",
    "        print('estimating lag model C' + str(CHUNK + 1) + 'R' + str(ROW + 1) + ' of C' + str(numChunks) + 'R' + str(pairMap.shape[0]))\n",
    "        print('DBIC participant: ' + pairMap['dbicID'][ROW])\n",
    "        print('CBS participant: ' + pairMap['cbsID'][ROW])\n",
    "        print('storytelling condition: ' + storyConds[pairMap['condition'][ROW]])\n",
    "        print('pair type: ' + pairTypes[pairMap['pairType'][ROW]])\n",
    "        print('speaker: ' + speakers[pairMap['dbicSpeaker'][ROW]] + '\\n')\n",
    "\n",
    "        # load speaker timeseries\n",
    "        sFile = pairMap['sFile'][ROW]\n",
    "        if os.path.isfile(sFile): # if there is a file to load...\n",
    "            dummyFile = sio.loadmat(sFile) # load file\n",
    "            if pairMap['dbicSpeaker'][ROW] == 1:\n",
    "                speaker = dummyFile['dbicSpeaker'][:,voxelCoords] # get timeseries data\n",
    "            else:\n",
    "                speaker = dummyFile['cbsSpeaker'][:,voxelCoords] # get timeseries data\n",
    "            del dummyFile\n",
    "        else:\n",
    "            print('WARNING! ' + sFile + ' not found!')\n",
    "\n",
    "        # load listener timeseries\n",
    "        lFile = pairMap['lFile'][ROW]\n",
    "        if os.path.isfile(lFile):\n",
    "            dummy = sio.loadmat(lFile) # load data\n",
    "            if pairMap['dbicSpeaker'][ROW] == 1:\n",
    "                listener = dummy['cbsListener'][:,voxelCoords] # get time series data\n",
    "            else:\n",
    "                listener = dummy['dbicListener'][:,voxelCoords] # get time series data\n",
    "            del dummy\n",
    "        else:\n",
    "            print('WARNING! ' + lFile + ' not found!')\n",
    "\n",
    "        # subset to current voxel chunk\n",
    "        speaker = speaker[:,chunkInds[CHUNK]]\n",
    "        listener = listener[:,chunkInds[CHUNK]]\n",
    "        numVox = len(chunkInds[CHUNK])\n",
    "\n",
    "        # for each modeling window...\n",
    "        for STEP in range(numSteps):\n",
    "\n",
    "            # initialize dictionaries for ISC variables\n",
    "            iscData[ROW][STEP] = {'betas':np.empty([numVox,maxT*2+1]),\n",
    "                                  'resids':np.empty(numVox),\n",
    "                                  'R':np.empty(numVox),\n",
    "                                  'R_pVals':dict(),\n",
    "                                  'R_masks':dict(),\n",
    "                                  'R_pSig':dict()}\n",
    "            perms[ROW][STEP] = {'betas':np.empty([numVox,maxT*2+1,permuts]),\n",
    "                                'resids':np.empty([numVox,permuts]),\n",
    "                                'R':np.empty([numVox,permuts])}\n",
    "\n",
    "            # get subset of TRs corresponding to current rolling window step\n",
    "            s_trunc = speaker[stepTRs[STEP],:]\n",
    "            l_trunc = listener[stepTRs[STEP],:]\n",
    "\n",
    "            # run real coupling model\n",
    "\n",
    "            iscData[ROW][STEP]['betas'], iscData[ROW][STEP]['resids'], iscData[ROW][STEP]['R'] = coupling_onestep(s_trunc,l_trunc,maxT,padding)\n",
    "\n",
    "            ########################\n",
    "            ### permutation test ###\n",
    "            ########################\n",
    "\n",
    "            # if not running parallel\n",
    "            if not parallel:\n",
    "\n",
    "                # for each permutation...\n",
    "                for PERM in range(permuts):\n",
    "\n",
    "                    # run coupling model with scrambled time series\n",
    "                    if circShift: # circle shifting...\n",
    "                        perms[ROW][STEP]['betas'][:,:,PERM], perms[ROW][STEP]['resids'][:,[PERM]], perms[ROW][STEP]['R'][:,[PERM]] = coupling_onestep(Circle_Shift_3(s_trunc,shuffleRange=shuffleRange),l_trunc,maxT,padding)\n",
    "                    else: # phase scrambling...\n",
    "                        perms[ROW][STEP]['betas'][:,:,PERM], perms[ROW][STEP]['resids'][:,[PERM]], perms[ROW][STEP]['R'][:,[PERM]] = coupling_onestep(phase_scrambling_2(s_trunc,shuffleRange=shuffleRange),l_trunc,maxT,padding)\n",
    "\n",
    "            # if running ISC in parallel across permutations\n",
    "            else:\n",
    "\n",
    "                # run coupling model with scrambled time series with permutations in parallel\n",
    "                if circShift: # circle shifting...\n",
    "                    tmp = Parallel(n_jobs=numJobs, verbose=verbosity)(delayed(coupling_onestep)\n",
    "                                                  (Circle_Shift_3(s_trunc,shuffleRange=shuffleRange),\n",
    "                                                   l_trunc,\n",
    "                                                   maxT,\n",
    "                                                   padding)\n",
    "                                                  for PERM in range(permuts))\n",
    "                else: # phase scrambling\n",
    "                    tmp = Parallel(n_jobs=numJobs, verbose=verbosity)(delayed(coupling_onestep)\n",
    "                                                  (phase_scrambling_2(s_trunc,shuffleRange=shuffleRange),\n",
    "                                                   l_trunc,\n",
    "                                                   maxT,\n",
    "                                                   padding)\n",
    "                                                  for PERM in range(permuts))\n",
    "\n",
    "                # unpack joblib output\n",
    "                for PERM in range(permuts):\n",
    "                    perms[ROW][STEP]['betas'][:,:,PERM] = tmp[PERM][0] # beta coefficients\n",
    "                    perms[ROW][STEP]['resids'][:,[PERM]] = tmp[PERM][1] # residuals\n",
    "                    perms[ROW][STEP]['R'][:,[PERM]] = tmp[PERM][2] # R^2\n",
    "\n",
    "\n",
    "\n",
    "            # run permutation test\n",
    "            iscData[ROW][STEP]['R_pVals'], iscData[ROW][STEP]['R_masks'], iscData[ROW][STEP]['R_pSig'] = coupling_lm_permTest_v2(iscData[ROW][STEP]['R'], perms[ROW][STEP]['R'], alpha)\n",
    "\n",
    "            # print proportion of voxels with signficant FDR-corrected R^2 values at each step\n",
    "            print('Step ' + str(STEP+1) + ' of ' + str(numSteps) + ': TRs ' + str(np.min(stepTRs[STEP])) + ' to ' + str(np.max(stepTRs[STEP])))\n",
    "            print('% of voxels with signficant FDR-corrected R^2 values: ' + str(round(iscData[ROW][STEP]['R_pSig']['two'] * 100,2)) + '%')\n",
    "\n",
    "        # if using more than one modeling window...\n",
    "        if numSteps > 1:\n",
    "\n",
    "            # initialize dictionaries for parameter estimate averages\n",
    "            paramAvg[ROW] = {'betas':np.empty([numVox,maxT*2+1]),\n",
    "                             'resids':np.empty(numVox),\n",
    "                             'R':np.empty(numVox),\n",
    "                             'R_pVals':dict(),\n",
    "                             'R_masks':dict(),\n",
    "                             'R_pSig':dict()}\n",
    "            perms_paramAvg[ROW] = {'betas':np.empty([numVox,maxT*2+1,permuts]),\n",
    "                                   'resids':np.empty([numVox,permuts]),\n",
    "                                   'R':np.empty([numVox,permuts])}\n",
    "\n",
    "            # get averages parameter estimates across modeling windows\n",
    "            paramAvg[ROW]['betas'] = np.mean([iscData[ROW][STEP]['betas'] for STEP in range(numSteps)], axis=0) # mean beta coefficients\n",
    "            paramAvg[ROW]['resids'] = np.mean([iscData[ROW][STEP]['resids'] for STEP in range(numSteps)], axis=0) # mean residuals\n",
    "            paramAvg[ROW]['R'] = np.mean([iscData[ROW][STEP]['R'] for STEP in range(numSteps)], axis=0) # mean R^2\n",
    "            perms_paramAvg[ROW]['betas'] = np.mean([perms[ROW][STEP]['betas'] for STEP in range(numSteps)], axis=0) # permuted beta coefficients\n",
    "            perms_paramAvg[ROW]['resids'] = np.mean([perms[ROW][STEP]['resids'] for STEP in range(numSteps)],axis=0) # permuted residuals\n",
    "            perms_paramAvg[ROW]['R'] = np.mean([perms[ROW][STEP]['R'] for STEP in range(numSteps)],axis=0) # permuted mean R^2\n",
    "\n",
    "            # run permutation test on averaged parameter estimates\n",
    "            paramAvg[ROW]['R_pVals'], paramAvg[ROW]['R_masks'], paramAvg[ROW]['R_pSig'] = coupling_lm_permTest_v2(paramAvg[ROW]['R'], perms_paramAvg[ROW]['R'], alpha)\n",
    "\n",
    "            # print proportion of voxels with signficant FDR-corrected R^2 values\n",
    "            print('\\nAcross steps:')\n",
    "            print('% of voxels with signficant FDR-corrected mean R^2 values: ' + str(round(paramAvg[ROW]['R_pSig']['two'] * 100,2)) + '%')\n",
    "\n",
    "        # get duration for current row\n",
    "        pairMap['duration'][ROW] = round((time.time() - rowStart) / 60,2)\n",
    "        print('processing time for model ' + str(ROW+1) + ': ~' + str(pairMap['duration'][ROW]) + ' min\\n')\n",
    "\n",
    "    ######################################################################################\n",
    "    ### Get lag model computation duration and mean processing time per row of pairMap ###\n",
    "    ######################################################################################\n",
    "\n",
    "    lagComp_total = round(time.time() - startTime / 60, 2) # [min]\n",
    "    lagComp_mean_per_pairMap_row = pairMap['duration'].mean()\n",
    "\n",
    "    ####################################################################################################\n",
    "    ### get joint - independent contrast for each unique pair (averaging across speaker orientation) ###\n",
    "    ####################################################################################################\n",
    "\n",
    "    # preallocate array for unique pairs\n",
    "    jVSi = [[]] * len(pairInds)\n",
    "    perms_jVSi = [[]] * len(pairInds)\n",
    "    if numSteps > 1:\n",
    "        jVSi_paramAvg = [[]] * len(pairInds)\n",
    "        perms_jVSi_paramAvg = [[]] * len(pairInds)\n",
    "    else:\n",
    "        jVSi_paramAvg = np.nan\n",
    "        perms_jVSi_paramAvg = np.nan\n",
    "\n",
    "    # for each unique pair...\n",
    "    for PAIR in range(len(pairInds)):\n",
    "\n",
    "        # preallocate step-wise lists\n",
    "        jVSi[PAIR] = [[]] * numSteps\n",
    "        perms_jVSi[PAIR] = [[]] * numSteps\n",
    "\n",
    "        # for each modeling window...\n",
    "        for STEP in range(numSteps):\n",
    "\n",
    "            # preallocate arrays for mean R^2 values across speaker orientations\n",
    "            mean_R = [[]] * 2 # real mean\n",
    "            mean_R_perms = [[]] * 2 # permutation test means\n",
    "\n",
    "            # for each condition (0=independent, 1=joint)...\n",
    "            for COND in [0,1]:\n",
    "\n",
    "                # average across speaker orientation\n",
    "                mean_R[COND] = np.mean([iscData[pairInds[PAIR][COND][0]][STEP]['R'], iscData[pairInds[PAIR][COND][1]][STEP]['R']], axis=0)\n",
    "                mean_R_perms[COND] = np.mean([perms[pairInds[PAIR][COND][0]][STEP]['R'], perms[pairInds[PAIR][COND][1]][STEP]['R']], axis=0)\n",
    "\n",
    "            # initialize dictionaries (joint - independent) permutation tests\n",
    "            jVSi[PAIR][STEP] = {'R':np.empty(numVox),\n",
    "                                'R_pVals':dict(),\n",
    "                                'R_masks':dict(),\n",
    "                                'R_pSig':dict()}\n",
    "            perms_jVSi[PAIR][STEP] = {'R':np.empty([numVox,permuts])}\n",
    "\n",
    "            # get difference scores\n",
    "            jVSi[PAIR][STEP]['R'] = mean_R[1] - mean_R[0]\n",
    "            perms_jVSi[PAIR][STEP]['R'] = mean_R_perms[1] - mean_R_perms[0]\n",
    "\n",
    "            # run permutation test\n",
    "            jVSi[PAIR][STEP]['R_pVals'], jVSi[PAIR][STEP]['R_masks'], jVSi[PAIR][STEP]['R_pSig'] = coupling_lm_permTest_v2(jVSi[PAIR][STEP]['R'], perms_jVSi[PAIR][STEP]['R'], alpha)\n",
    "\n",
    "        # if using more than one modeling window...\n",
    "        if numSteps > 1:\n",
    "\n",
    "            # preallocate arrays for mean R^2 values across speaker orientations\n",
    "            mean_R = [[]] * 2 # real mean\n",
    "            mean_R_perms = [[]] * 2 # permutation test means\n",
    "\n",
    "            # for each condition (0=independent, 1=joint)...\n",
    "            for COND in [0,1]:\n",
    "\n",
    "                # average across speaker orientation\n",
    "                mean_R[COND] = np.mean([paramAvg[pairInds[PAIR][COND][0]]['R'], paramAvg[pairInds[PAIR][COND][1]]['R']], axis=0)\n",
    "                mean_R_perms[COND] = np.mean([perms_paramAvg[pairInds[PAIR][COND][0]]['R'], perms_paramAvg[pairInds[PAIR][COND][1]]['R']], axis=0)\n",
    "\n",
    "            # initialize dictionaries (joint - independent) permutation tests\n",
    "            jVSi_paramAvg[PAIR] = {'R':np.empty(numVox),\n",
    "                                   'R_pVals':dict(),\n",
    "                                   'R_masks':dict(),\n",
    "                                   'R_pSig':dict()}\n",
    "            perms_jVSi_paramAvg[PAIR] = {'R':np.empty([numVox,permuts])}\n",
    "\n",
    "            # get difference scores\n",
    "            jVSi_paramAvg[PAIR]['R'] = mean_R[1] - mean_R[0]\n",
    "            perms_jVSi_paramAvg[PAIR]['R'] = mean_R_perms[1] - mean_R_perms[0]\n",
    "\n",
    "            # run permutation test\n",
    "            jVSi_paramAvg[PAIR]['R_pVals'], jVSi_paramAvg[PAIR]['R_masks'], jVSi_paramAvg[PAIR]['R_pSig'] = coupling_lm_permTest_v2(jVSi_paramAvg[PAIR]['R'], perms_jVSi_paramAvg[PAIR]['R'], alpha)\n",
    "\n",
    "\n",
    "    ###################################################\n",
    "    ### Group level stats for conditions separately ###\n",
    "    ###################################################\n",
    "\n",
    "    # preallocate\n",
    "    condInds = [[]] * 2 # arrays for pairMap row indices for each of the independent and joint conditions\n",
    "    groupData = [[]] * 3 # condition-speicific arrays for stepwise permutation tests at the group level\n",
    "    if numSteps > 1:\n",
    "        groupData_paramAvg = [[]] * 3 # condition-specific arrays for permutation tests on average parameters (across steps) at the group level\n",
    "    else:\n",
    "        groupData_paramAvg = np.nan\n",
    "\n",
    "    # for each condition (0=independent, 1=joint, 2=joint minus independent)\n",
    "    for COND in range(3):\n",
    "\n",
    "        # get row indices from pairMap corresponding to each condition\n",
    "        if COND < 2:\n",
    "            condInds[COND] = np.where(pairMap['condition'] == COND)[0]\n",
    "\n",
    "        # preallocate\n",
    "        groupData[COND] = [[]] * numSteps\n",
    "\n",
    "        for STEP in range(numSteps):\n",
    "\n",
    "            # initialize dictionary\n",
    "            groupData[COND][STEP] = {'R_median':np.empty(numVox),\n",
    "                                     'R_median_pVals':dict(),\n",
    "                                     'R_median_masks':dict(),\n",
    "                                     'R_median_pSig':dict(),\n",
    "                                     'R_mean':np.empty(numVox),\n",
    "                                     'R_mean_pVals':dict(),\n",
    "                                     'R_mean_masks':dict(),\n",
    "                                     'R_mean_pSig':dict()}\n",
    "\n",
    "            # if independent or joint condition...\n",
    "            if COND < 2:\n",
    "\n",
    "                # compute means and medians\n",
    "                groupData[COND][STEP]['R_median'] = np.median([iscData[ROW][STEP]['R'] for ROW in condInds[COND]], axis=0)\n",
    "                groupData[COND][STEP]['R_mean'] = np.mean([iscData[ROW][STEP]['R'] for ROW in condInds[COND]], axis=0)\n",
    "                perms_median = np.median([perms[ROW][STEP]['R'] for ROW in condInds[COND]], axis=0)\n",
    "                perms_mean = np.mean([perms[ROW][STEP]['R'] for ROW in condInds[COND]], axis=0)\n",
    "\n",
    "            # if joint minus independent condition...\n",
    "            else:\n",
    "\n",
    "                # compute means and medians\n",
    "                groupData[COND][STEP]['R_median'] = np.median([jVSi[PAIR][STEP]['R'] for PAIR in range(len(jVSi))], axis=0)\n",
    "                groupData[COND][STEP]['R_mean'] = np.mean([jVSi[PAIR][STEP]['R'] for PAIR in range(len(jVSi))], axis=0)\n",
    "                perms_median = np.median([perms_jVSi[PAIR][STEP]['R'] for PAIR in range(len(perms_jVSi))], axis=0)\n",
    "                perms_mean = np.mean([perms_jVSi[PAIR][STEP]['R'] for PAIR in range(len(perms_jVSi))], axis=0)\n",
    "\n",
    "            # run permutation tests\n",
    "            groupData[COND][STEP]['R_median_pVals'], groupData[COND][STEP]['R_median_masks'], groupData[COND][STEP]['R_median_pSig'] = coupling_lm_permTest_v2(groupData[COND][STEP]['R_median'], perms_median, alpha)\n",
    "            groupData[COND][STEP]['R_mean_pVals'], groupData[COND][STEP]['R_mean_masks'], groupData[COND][STEP]['R_mean_pSig'] = coupling_lm_permTest_v2(groupData[COND][STEP]['R_mean'], perms_mean, alpha)\n",
    "\n",
    "            # feedback about group median R^2 values\n",
    "            print('\\n% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in top ' + str(round((alpha / 2) * 100,2)) + '% of null distribution')\n",
    "            print('in step ' + str(STEP+1) + ' of the ' + storyConds[COND] + ' condition: ' + str(round(groupData[COND][STEP]['R_median_pSig']['two_above'] * 100,2)) + '%')\n",
    "            print('\\n% of voxels where group MEDIAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom ' + str(round((alpha / 2) * 100,2)) + '% of null distribution')\n",
    "            print('in step ' + str(STEP+1) + ' of the ' + storyConds[COND] + ' condition: ' + str(round(groupData[COND][STEP]['R_median_pSig']['two_below'] * 100,2)) + '%')\n",
    "\n",
    "            # feedback about group mean R^2 values\n",
    "            print('\\n% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in top ' + str(round((alpha / 2) * 100,2)) + '% of null distribution')\n",
    "            print('in step ' + str(STEP+1) + ' of the ' + storyConds[COND] + ' condition: ' + str(round(groupData[COND][STEP]['R_mean_pSig']['two_above'] * 100,2)) + '%')\n",
    "            print('\\n% of voxels where group MEAN (across pairs) mean (across speech turns within pairs) R^2 values is in bottom ' + str(round((alpha / 2) * 100,2)) + '% of null distribution')\n",
    "            print('in step ' + str(STEP+1) + ' of the ' + storyConds[COND] + ' condition: ' + str(round(groupData[COND][STEP]['R_mean_pSig']['two_below'] * 100,2)) + '%')\n",
    "\n",
    "        # if using more than one modeling window...\n",
    "        if numSteps > 1:\n",
    "\n",
    "            # initialize dictionary\n",
    "            groupData_paramAvg[COND] = {'R_median':np.empty(numVox),\n",
    "                                        'R_median_pVals':dict(),\n",
    "                                        'R_median_masks':dict(),\n",
    "                                        'R_median_pSig':dict(),\n",
    "                                        'R_mean':np.empty(numVox),\n",
    "                                        'R_mean_pVals':dict(),\n",
    "                                        'R_mean_masks':dict(),\n",
    "                                        'R_mean_pSig':dict()}\n",
    "\n",
    "            # if independent or joint condition...\n",
    "            if COND < 2:\n",
    "\n",
    "                # compute means and medians\n",
    "                groupData_paramAvg[COND]['R_median'] = np.median([paramAvg[ROW]['R'] for ROW in condInds[COND]], axis=0)\n",
    "                groupData_paramAvg[COND]['R_mean'] = np.mean([paramAvg[ROW]['R'] for ROW in condInds[COND]], axis=0)\n",
    "                perms_median = np.median([perms_paramAvg[ROW]['R'] for ROW in condInds[COND]], axis=0)\n",
    "                perms_mean = np.mean([perms_paramAvg[ROW]['R'] for ROW in condInds[COND]], axis=0)\n",
    "\n",
    "            # if joint minus independent condition...\n",
    "            else:\n",
    "\n",
    "                # compute means and medians\n",
    "                groupData_paramAvg[COND]['R_median'] = np.median([jVSi_paramAvg[PAIR]['R'] for PAIR in range(len(jVSi_paramAvg))], axis=0)\n",
    "                groupData_paramAvg[COND]['R_mean'] = np.mean([jVSi_paramAvg[PAIR]['R'] for PAIR in range(len(jVSi_paramAvg))], axis=0)\n",
    "                perms_median = np.median([perms_jVSi_paramAvg[PAIR]['R'] for ROW in range(len(perms_jVSi_paramAvg))], axis=0)\n",
    "                perms_mean = np.mean([perms_jVSi_paramAvg[PAIR]['R'] for ROW in range(len(perms_jVSi_paramAvg))], axis=0)\n",
    "\n",
    "            # run permutation tests\n",
    "            groupData_paramAvg[COND]['R_median_pVals'], groupData_paramAvg[COND]['R_median_masks'], groupData_paramAvg[COND]['R_median_pSig'] = coupling_lm_permTest_v2(groupData_paramAvg[COND]['R_median'], perms_median, alpha)\n",
    "            groupData_paramAvg[COND]['R_mean_pVals'], groupData_paramAvg[COND]['R_mean_masks'], groupData_paramAvg[COND]['R_mean_pSig'] = coupling_lm_permTest_v2(groupData_paramAvg[COND]['R_mean'], perms_mean, alpha)\n",
    "\n",
    "            # feedback about group median R^2 values\n",
    "            print('\\n% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top ' + str(round((alpha / 2) * 100,2)) + '% of null distribution')\n",
    "            print('in step ' + str(STEP+1) + ' of the ' + storyConds[COND] + ' condition: ' + str(round(groupData_paramAvg[COND]['R_median_pSig']['two_above'] * 100,2)) + '%')\n",
    "            print('\\n% of voxels where GROUP MEDIAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom ' + str(round((alpha / 2) * 100,2)) + '% of null distribution')\n",
    "            print('in step ' + str(STEP+1) + ' of the ' + storyConds[COND] + ' condition: ' + str(round(groupData_paramAvg[COND]['R_median_pSig']['two_below'] * 100,2)) + '%')\n",
    "\n",
    "            # feedback about group mean R^2 values\n",
    "            print('\\n% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in top ' + str(round((alpha / 2) * 100,2)) + '% of null distribution')\n",
    "            print('in step ' + str(STEP+1) + ' of the ' + storyConds[COND] + ' condition: ' + str(round(groupData_paramAvg[COND]['R_mean_pSig']['two_above'] * 100,2)) + '%')\n",
    "            print('\\n% of voxels where GROUP MEAN (across pairs) mean (across steps) mean (across speech turns within pairs) R^2 values is in bottom ' + str(round((alpha / 2) * 100,2)) + '% of null distribution')\n",
    "            print('in step ' + str(STEP+1) + ' of the ' + storyConds[COND] + ' condition: ' + str(round(groupData_paramAvg[COND]['R_mean_pSig']['two_below'] * 100,2)) + '%')\n",
    "\n",
    "    ###########################################################\n",
    "    ### store data from current voxel chunk in dictionaries ###\n",
    "    ###########################################################\n",
    "\n",
    "    # duration\n",
    "    chunkDur[CHUNK] = {'lagComp_total':lagComp_total,\n",
    "           'lagComp_mean_per_pairMap_row':lagComp_mean_per_pairMap_row}\n",
    "    pairData_by_step[CHUNK] = iscData\n",
    "    pairData_across_steps[CHUNK] = paramAvg\n",
    "    pairData_jVSi_by_step[CHUNK] = jVSi\n",
    "    pairData_jVSi_across_steps[CHUNK] = jVSi_paramAvg\n",
    "    groupData_by_step[CHUNK] = groupData\n",
    "    groupData_across_steps[CHUNK] = groupData_paramAvg\n",
    "\n",
    "    # delete variables to save memory\n",
    "    del speaker, listener, iscData, paramAvg, jVSi, jVSi_paramAvg, groupData, groupData_paramAvg, \\\n",
    "        perms, perms_paramAvg, perms_jVSi, perms_jVSi_paramAvg, perms_median, perms_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Run the linear models\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DBIC sub 2, CBS sub 2, independent condition\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 21.66%\n",
      "\n",
      "DBIC sub 2, CBS sub 2, independent condition\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 16.23%\n",
      "\n",
      "DBIC sub 3, CBS sub 3, independent condition\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 15.71%\n",
      "\n",
      "DBIC sub 3, CBS sub 3, independent condition\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 18.42%\n",
      "\n",
      "DBIC sub 3, CBS sub 3, independent condition, step 1\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 17.93%\n",
      "\n",
      "DBIC sub 3, CBS sub 3, independent condition, step 2\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 19.47%\n",
      "\n",
      "Group data, independent condition\n",
      "% of voxels with signficant FDR-corrected MEDIAN (across pairs) mean (across steps) R^2 values: 17.88%\n",
      "% of voxels with signficant FDR-corrected MEAN (across pairs) mean (across steps) R^2 values: 17.82%\n",
      "\n",
      "Group data, independent condition, step 1\n",
      "% of voxels with signficant FDR-corrected MEDIAN (across pairs) mean (across steps) R^2 values: 17.77%\n",
      "% of voxels with signficant FDR-corrected MEAN (across pairs) mean (across steps) R^2 values: 17.01%\n",
      "\n",
      "Group data, independent condition, step 2\n",
      "% of voxels with signficant FDR-corrected MEDIAN (across pairs) mean (across steps) R^2 values: 18.76%\n",
      "% of voxels with signficant FDR-corrected MEAN (across pairs) mean (across steps) R^2 values: 18.87%\n",
      "\n",
      "DBIC sub 2, CBS sub 2, joint condition\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 20.24%\n",
      "\n",
      "DBIC sub 2, CBS sub 2, joint condition\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 18.06%\n",
      "\n",
      "DBIC sub 3, CBS sub 3, joint condition\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 17.88%\n",
      "\n",
      "DBIC sub 3, CBS sub 3, joint condition\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 18.11%\n",
      "\n",
      "DBIC sub 3, CBS sub 3, joint condition, step 1\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 18.17%\n",
      "\n",
      "DBIC sub 3, CBS sub 3, joint condition, step 2\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 18.42%\n",
      "\n",
      "Group data, joint condition\n",
      "% of voxels with signficant FDR-corrected MEDIAN (across pairs) mean (across steps) R^2 values: 18.23%\n",
      "% of voxels with signficant FDR-corrected MEAN (across pairs) mean (across steps) R^2 values: 18.63%\n",
      "\n",
      "Group data, joint condition, step 1\n",
      "% of voxels with signficant FDR-corrected MEDIAN (across pairs) mean (across steps) R^2 values: 18.0%\n",
      "% of voxels with signficant FDR-corrected MEAN (across pairs) mean (across steps) R^2 values: 17.94%\n",
      "\n",
      "Group data, joint condition, step 2\n",
      "% of voxels with signficant FDR-corrected MEDIAN (across pairs) mean (across steps) R^2 values: 19.13%\n",
      "% of voxels with signficant FDR-corrected MEAN (across pairs) mean (across steps) R^2 values: 19.62%\n",
      "\n",
      "DBIC sub 2, CBS sub 2, joint minus independent condition\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 19.05%\n",
      "\n",
      "DBIC sub 2, CBS sub 2, joint minus independent condition\n",
      "% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: 17.23%\n",
      "\n",
      "Group data, joint minus independent condition\n",
      "% of voxels with signficant FDR-corrected MEDIAN (across pairs) mean (across steps) R^2 values: 30.04%\n",
      "% of voxels with signficant FDR-corrected MEAN (across pairs) mean (across steps) R^2 values: 30.04%\n",
      "\n",
      "Group data, joint minus independent condition, step 1\n",
      "% of voxels with signficant FDR-corrected MEDIAN (across pairs) mean (across steps) R^2 values: 17.35%\n",
      "% of voxels with signficant FDR-corrected MEAN (across pairs) mean (across steps) R^2 values: 17.35%\n",
      "\n",
      "Group data, joint minus independent condition, step 2\n",
      "% of voxels with signficant FDR-corrected MEDIAN (across pairs) mean (across steps) R^2 values: 19.0%\n",
      "% of voxels with signficant FDR-corrected MEAN (across pairs) mean (across steps) R^2 values: 19.0%\n"
     ]
    }
   ],
   "source": [
    "# preallocate conditions arrays (1=independent, 2=joint, 3=joint-independent)\n",
    "pairDataByStep = [[]] * len(storyConds)\n",
    "groupDataByStep = [[]] * len(storyConds)\n",
    "pairDataAcrossSteps = [[]] * len(storyConds)\n",
    "groupDataAcrossSteps = [[]] * len(storyConds)\n",
    "\n",
    "# for each condition (independent, joint)...\n",
    "for COND in range(len(storyConds)):\n",
    "\n",
    "    # preallocate pair arrays\n",
    "    pairDataByStep[COND] = [[]] * len(condInds[0])\n",
    "    if numSteps > 1:\n",
    "        pairDataAcrossSteps[COND] = [[]] * len(condInds[0])\n",
    "\n",
    "    # for each chunk of voxels...\n",
    "    for CHUNK in range(numChunks):\n",
    "\n",
    "        ########################################################\n",
    "        ### pair-level mean parameter estimates across steps ###\n",
    "        ########################################################\n",
    "\n",
    "        if numSteps > 1:\n",
    "\n",
    "            # for each set of pair data for a given condition...\n",
    "            for PAIR in range(len(condInds[0])):\n",
    "\n",
    "                # if on the first chunk...\n",
    "                if CHUNK == 0:\n",
    "\n",
    "                    # as long as we're not on the joint minus independent condition past the number of unique pairs...\n",
    "                    if not (COND == 2 and PAIR >= uniquePairs.shape[0]):\n",
    "\n",
    "                        # initialize across-steps dictionary\n",
    "                        pairDataAcrossSteps[COND][PAIR] = {'betas':np.empty([totalVox,maxT*2+1]),\n",
    "                                                           'resids':np.empty(totalVox),\n",
    "                                                           'R':np.empty(totalVox),\n",
    "                                                           'R_pVals':dict(),\n",
    "                                                           'R_masks':dict(),\n",
    "                                                           'R_pSig':dict()}\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_pVals'] = {'right_tail':np.empty(totalVox),\n",
    "                                                                      'left_tail':np.empty(totalVox),\n",
    "                                                                      'two_tail':np.empty(totalVox),\n",
    "                                                                      'map':np.zeros(totalVox, dtype=bool)}\n",
    "\n",
    "                # if independent or joint condition...\n",
    "                if COND < 2:\n",
    "\n",
    "                    # get across-steps data\n",
    "                    pairDataAcrossSteps[COND][PAIR]['betas'][chunkInds[CHUNK]] = pairData_across_steps[CHUNK][condInds[COND][PAIR]]['betas']\n",
    "                    pairDataAcrossSteps[COND][PAIR]['resids'][chunkInds[CHUNK]] = np.squeeze(pairData_across_steps[CHUNK][condInds[COND][PAIR]]['resids'])\n",
    "                    pairDataAcrossSteps[COND][PAIR]['R'][chunkInds[CHUNK]] = np.squeeze(pairData_across_steps[CHUNK][condInds[COND][PAIR]]['R'])\n",
    "                    pairDataAcrossSteps[COND][PAIR]['R_pVals']['right_tail'][chunkInds[CHUNK]] = np.squeeze(pairData_across_steps[CHUNK][condInds[COND][PAIR]]['R_pVals']['right_tail'])\n",
    "                    pairDataAcrossSteps[COND][PAIR]['R_pVals']['left_tail'][chunkInds[CHUNK]] = np.squeeze(pairData_across_steps[CHUNK][condInds[COND][PAIR]]['R_pVals']['left_tail'])\n",
    "                    pairDataAcrossSteps[COND][PAIR]['R_pVals']['two_tail'][chunkInds[CHUNK]] = np.squeeze(pairData_across_steps[CHUNK][condInds[COND][PAIR]]['R_pVals']['two_tail'])\n",
    "                    pairDataAcrossSteps[COND][PAIR]['R_pVals']['map'][chunkInds[CHUNK]] = np.squeeze(pairData_across_steps[CHUNK][condInds[COND][PAIR]]['R_pVals']['map'])\n",
    "\n",
    "                # if joint minus independent condition...\n",
    "                else:\n",
    "\n",
    "                    # if we haven't already gone through the total number of unique pairs\n",
    "                    if PAIR < uniquePairs.shape[0]:\n",
    "\n",
    "                        # get across-steps data\n",
    "                        pairDataAcrossSteps[COND][PAIR]['betas'][chunkInds[CHUNK]] = np.nan # presumably we're not interested in the difference between the betas\n",
    "                        pairDataAcrossSteps[COND][PAIR]['resids'][chunkInds[CHUNK]] = np.nan # presumably we're not interested in the difference between the residuals\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R'][chunkInds[CHUNK]] = np.squeeze(pairData_jVSi_across_steps[CHUNK][PAIR]['R'])\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_pVals']['right_tail'][chunkInds[CHUNK]] = np.squeeze(pairData_jVSi_across_steps[CHUNK][PAIR]['R_pVals']['right_tail'])\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_pVals']['left_tail'][chunkInds[CHUNK]] = np.squeeze(pairData_jVSi_across_steps[CHUNK][PAIR]['R_pVals']['left_tail'])\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_pVals']['two_tail'][chunkInds[CHUNK]] = np.squeeze(pairData_jVSi_across_steps[CHUNK][PAIR]['R_pVals']['two_tail'])\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_pVals']['map'][chunkInds[CHUNK]] = np.squeeze(pairData_jVSi_across_steps[CHUNK][PAIR]['R_pVals']['map'])\n",
    "\n",
    "                # if on the last chunk...\n",
    "                if CHUNK == numChunks-1:\n",
    "\n",
    "                    # as long as we're not on the joint minus independent condition past the number of unique pairs...\n",
    "                    if not (COND == 2 and PAIR >= uniquePairs.shape[0]):\n",
    "\n",
    "                        # reapply false discovery rate correction across all voxels\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_pVals']['fdr_right'] = multi.fdrcorrection(pairDataAcrossSteps[COND][PAIR]['R_pVals']['right_tail'], alpha = alpha)\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_pVals']['fdr_left'] = multi.fdrcorrection(pairDataAcrossSteps[COND][PAIR]['R_pVals']['left_tail'], alpha = alpha)\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_pVals']['fdr_two'] = multi.fdrcorrection(pairDataAcrossSteps[COND][PAIR]['R_pVals']['two_tail'], alpha = alpha / 2)\n",
    "\n",
    "                        # get R^2 masks where all voxels that did not survive FDR correction are set to zero\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_masks']['right'] = np.copy(pairDataAcrossSteps[COND][PAIR]['R'])\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_masks']['right'][np.invert(pairDataAcrossSteps[COND][PAIR]['R_pVals']['fdr_right'][0])] = 0\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_masks']['left'] = np.copy(pairDataAcrossSteps[COND][PAIR]['R'])\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_masks']['left'][np.invert(pairDataAcrossSteps[COND][PAIR]['R_pVals']['fdr_left'][0])] = 0\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_masks']['two'] = np.copy(pairDataAcrossSteps[COND][PAIR]['R'])\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_masks']['two'][np.invert(pairDataAcrossSteps[COND][PAIR]['R_pVals']['fdr_two'][0])] = 0\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_masks']['two_above'] = np.copy(pairDataAcrossSteps[COND][PAIR]['R'])\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_masks']['two_above'][np.minimum(pairDataAcrossSteps[COND][PAIR]['R_pVals']['fdr_two'][0], np.invert(pairDataAcrossSteps[COND][PAIR]['R_pVals']['map']))] = 0\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_masks']['two_below'] = np.copy(pairDataAcrossSteps[COND][PAIR]['R'])\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_masks']['two_below'][np.minimum(pairDataAcrossSteps[COND][PAIR]['R_pVals']['fdr_two'][0], pairDataAcrossSteps[COND][PAIR]['R_pVals']['map'])] = 0\n",
    "\n",
    "                        # get some FDR corrected p-value summary info\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_pSig']['right'] = len(np.where(pairDataAcrossSteps[COND][PAIR]['R_pVals']['fdr_right'][0])[0]) / totalVox\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_pSig']['left'] = len(np.where(pairDataAcrossSteps[COND][PAIR]['R_pVals']['fdr_left'][0])[0]) / totalVox\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_pSig']['two'] = len(np.where(pairDataAcrossSteps[COND][PAIR]['R_pVals']['fdr_two'][0])[0]) / totalVox\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_pSig']['two_above'] = np.count_nonzero(np.minimum(pairDataAcrossSteps[COND][PAIR]['R_pVals']['fdr_two'][0], np.invert(pairDataAcrossSteps[COND][PAIR]['R_pVals']['map']))) / totalVox # proportion of voxels that show significant FDR CORRECTED R^2 values above the median\n",
    "                        pairDataAcrossSteps[COND][PAIR]['R_pSig']['two_below'] = np.count_nonzero(np.minimum(pairDataAcrossSteps[COND][PAIR]['R_pVals']['fdr_two'][0], pairDataAcrossSteps[COND][PAIR]['R_pVals']['map'])) / totalVox\n",
    "\n",
    "                        # feedback\n",
    "                        print('\\nDBIC sub ' + str(pairMap['dbicNum'][condInds[0][PAIR]]) + ', CBS sub ' + str(pairMap['cbsNum'][condInds[0][PAIR]]) + ', ' + storyConds[COND] + ' condition')\n",
    "                        print('% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: ' + str(round(pairDataAcrossSteps[COND][PAIR]['R_pSig']['two'] * 100,2)) + '%')\n",
    "\n",
    "        else:\n",
    "\n",
    "            pairDataAcrossSteps = np.nan\n",
    "\n",
    "        ################################################\n",
    "        ### pair-level step-wise parameter estimates ###\n",
    "        ################################################\n",
    "\n",
    "        # preallocate step array\n",
    "        if CHUNK == 0:\n",
    "            pairDataByStep[COND][PAIR] = [[]] * numSteps\n",
    "\n",
    "        # for each rolling window step...\n",
    "        for STEP in range(numSteps):\n",
    "\n",
    "            # if it's the first chunk\n",
    "            if CHUNK == 0:\n",
    "\n",
    "                # initialize structure\n",
    "                pairDataByStep[COND][PAIR][STEP] = {'betas':np.empty([totalVox,maxT*2+1]),\n",
    "                                                    'resids':np.empty(totalVox),\n",
    "                                                    'R':np.empty(totalVox),\n",
    "                                                    'R_pVals':dict(),\n",
    "                                                    'R_masks':dict(),\n",
    "                                                    'R_pSig':dict()}\n",
    "                pairDataByStep[COND][PAIR][STEP]['R_pVals'] = {'right_tail':np.empty(totalVox),\n",
    "                                                              'left_tail':np.empty(totalVox),\n",
    "                                                              'two_tail':np.empty(totalVox),\n",
    "                                                              'map':np.zeros(totalVox, dtype=bool)}\n",
    "\n",
    "            # if independent or joint condition...\n",
    "            if COND < 2:\n",
    "\n",
    "                pairDataByStep[COND][PAIR][STEP]['betas'][chunkInds[CHUNK],:] = pairData_by_step[CHUNK][condInds[COND][PAIR]][STEP]['betas']\n",
    "                pairDataByStep[COND][PAIR][STEP]['resids'][chunkInds[CHUNK]] = np.squeeze(pairData_by_step[CHUNK][condInds[COND][PAIR]][STEP]['resids'])\n",
    "                pairDataByStep[COND][PAIR][STEP]['R'][chunkInds[CHUNK]] = np.squeeze(pairData_by_step[CHUNK][condInds[COND][PAIR]][STEP]['R'])\n",
    "                pairDataByStep[COND][PAIR][STEP]['R_pVals']['right_tail'][chunkInds[CHUNK]] = np.squeeze(pairData_by_step[CHUNK][condInds[COND][PAIR]][STEP]['R_pVals']['right_tail'])\n",
    "                pairDataByStep[COND][PAIR][STEP]['R_pVals']['left_tail'][chunkInds[CHUNK]] = np.squeeze(pairData_by_step[CHUNK][condInds[COND][PAIR]][STEP]['R_pVals']['left_tail'])\n",
    "                pairDataByStep[COND][PAIR][STEP]['R_pVals']['two_tail'][chunkInds[CHUNK]] = np.squeeze(pairData_by_step[CHUNK][condInds[COND][PAIR]][STEP]['R_pVals']['two_tail'])\n",
    "                pairDataByStep[COND][PAIR][STEP]['R_pVals']['map'][chunkInds[CHUNK]] = np.squeeze(pairData_by_step[CHUNK][condInds[COND][PAIR]][STEP]['R_pVals']['map'])\n",
    "\n",
    "            # if joint minus independent condition\n",
    "            else:\n",
    "\n",
    "                # if we haven't already gone through the total number of unique pairs\n",
    "                if PAIR < uniquePairs.shape[0]:\n",
    "\n",
    "                    pairDataByStep[COND][PAIR][STEP]['betas'][chunkInds[CHUNK]] = np.nan\n",
    "                    pairDataByStep[COND][PAIR][STEP]['resids'][chunkInds[CHUNK]] = np.nan\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R'][chunkInds[CHUNK]] = np.squeeze(pairData_jVSi_by_step[CHUNK][PAIR][STEP]['R'])\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_pVals']['right_tail'][chunkInds[CHUNK]] = np.squeeze(pairData_jVSi_by_step[CHUNK][PAIR][STEP]['R_pVals']['right_tail'])\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_pVals']['left_tail'][chunkInds[CHUNK]] = np.squeeze(pairData_jVSi_by_step[CHUNK][PAIR][STEP]['R_pVals']['left_tail'])\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_pVals']['two_tail'][chunkInds[CHUNK]] = np.squeeze(pairData_jVSi_by_step[CHUNK][PAIR][STEP]['R_pVals']['two_tail'])\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_pVals']['map'][chunkInds[CHUNK]] = np.squeeze(pairData_jVSi_by_step[CHUNK][PAIR][STEP]['R_pVals']['map'])\n",
    "\n",
    "            # if on the last chunk...\n",
    "            if CHUNK == numChunks-1:\n",
    "\n",
    "                # as long as we're not on the joint minus independent condition past the number of unique pairs...\n",
    "                if not (COND == 2 and PAIR >= uniquePairs.shape[0]):\n",
    "\n",
    "                    # reapply false discovery rate correction across all voxels\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_pVals']['fdr_right'] = multi.fdrcorrection(pairDataByStep[COND][PAIR][STEP]['R_pVals']['right_tail'], alpha = alpha)\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_pVals']['fdr_left'] = multi.fdrcorrection(pairDataByStep[COND][PAIR][STEP]['R_pVals']['left_tail'], alpha = alpha)\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_pVals']['fdr_two'] = multi.fdrcorrection(pairDataByStep[COND][PAIR][STEP]['R_pVals']['two_tail'], alpha = alpha / 2)\n",
    "\n",
    "                    # get R^2 masks where all voxels that did not survive FDR correction are set to zero\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_masks']['right'] = np.copy(pairDataByStep[COND][PAIR][STEP]['R'])\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_masks']['right'][np.invert(pairDataByStep[COND][PAIR][STEP]['R_pVals']['fdr_right'][0])] = 0\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_masks']['left'] = np.copy(pairDataByStep[COND][PAIR][STEP]['R'])\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_masks']['left'][np.invert(pairDataByStep[COND][PAIR][STEP]['R_pVals']['fdr_left'][0])] = 0\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_masks']['two'] = np.copy(pairDataByStep[COND][PAIR][STEP]['R'])\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_masks']['two'][np.invert(pairDataByStep[COND][PAIR][STEP]['R_pVals']['fdr_two'][0])] = 0\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_masks']['two_above'] = np.copy(pairDataByStep[COND][PAIR][STEP]['R'])\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_masks']['two_above'][np.minimum(pairDataByStep[COND][PAIR][STEP]['R_pVals']['fdr_two'][0], np.invert(pairDataByStep[COND][PAIR][STEP]['R_pVals']['map']))] = 0\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_masks']['two_below'] = np.copy(pairDataByStep[COND][PAIR][STEP]['R'])\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_masks']['two_below'][np.minimum(pairDataByStep[COND][PAIR][STEP]['R_pVals']['fdr_two'][0], pairDataByStep[COND][PAIR][STEP]['R_pVals']['map'])] = 0\n",
    "\n",
    "                    # get some FDR corrected p-value summary info\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_pSig']['right'] = len(np.where(pairDataByStep[COND][PAIR][STEP]['R_pVals']['fdr_right'][0])[0]) / totalVox\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_pSig']['left'] = len(np.where(pairDataByStep[COND][PAIR][STEP]['R_pVals']['fdr_left'][0])[0]) / totalVox\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_pSig']['two'] = len(np.where(pairDataByStep[COND][PAIR][STEP]['R_pVals']['fdr_two'][0])[0]) / totalVox\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_pSig']['two_above'] = np.count_nonzero(np.minimum(pairDataByStep[COND][PAIR][STEP]['R_pVals']['fdr_two'][0], np.invert(pairDataByStep[COND][PAIR][STEP]['R_pVals']['map']))) / totalVox # proportion of voxels that show significant FDR CORRECTED R^2 values above the median\n",
    "                    pairDataByStep[COND][PAIR][STEP]['R_pSig']['two_below'] = np.count_nonzero(np.minimum(pairDataByStep[COND][PAIR][STEP]['R_pVals']['fdr_two'][0], pairDataByStep[COND][PAIR][STEP]['R_pVals']['map'])) / totalVox\n",
    "\n",
    "                    # feedback\n",
    "                    print('\\nDBIC sub ' + str(pairMap['dbicNum'][condInds[0][PAIR]]) + ', CBS sub ' + str(pairMap['cbsNum'][condInds[0][PAIR]]) + ', ' + storyConds[COND] + ' condition, step ' + str(STEP+1))\n",
    "                    print('% of voxels wit`h signficant FDR-corrected mean (across steps) R^2 values: ' + str(round(pairDataByStep[COND][PAIR][STEP]['R_pSig']['two'] * 100,2)) + '%')\n",
    "\n",
    "        ###############################\n",
    "        ### group data across steps ###\n",
    "        ###############################\n",
    "\n",
    "        if numSteps > 1:\n",
    "\n",
    "            # if it's the first chunk\n",
    "            if CHUNK == 0:\n",
    "\n",
    "                # preallocate across-steps data dictionaries\n",
    "                groupDataAcrossSteps[COND] = {'R_median':np.empty(totalVox),\n",
    "                                              'R_median_pVals':dict(),\n",
    "                                              'R_median_masks':dict(),\n",
    "                                              'R_median_pSig':dict(),\n",
    "                                              'R_mean':np.empty(totalVox),\n",
    "                                              'R_mean_pVals':dict(),\n",
    "                                              'R_mean_masks':dict(),\n",
    "                                              'R_mean_pSig':dict()}\n",
    "                groupDataAcrossSteps[COND]['R_median_pVals'] = {'right_tail':np.empty(totalVox),\n",
    "                                                                'left_tail':np.empty(totalVox),\n",
    "                                                                'two_tail':np.empty(totalVox),\n",
    "                                                                'map':np.zeros(totalVox, dtype=bool)}\n",
    "                groupDataAcrossSteps[COND]['R_mean_pVals'] = {'right_tail':np.empty(totalVox),\n",
    "                                                              'left_tail':np.empty(totalVox),\n",
    "                                                              'two_tail':np.empty(totalVox),\n",
    "                                                              'map':np.zeros(totalVox, dtype=bool)}\n",
    "\n",
    "            # get group data by chunk\n",
    "            groupDataAcrossSteps[COND]['R_median'][chunkInds[CHUNK]] = np.squeeze(groupData_across_steps[CHUNK][COND]['R_median'])\n",
    "            groupDataAcrossSteps[COND]['R_median_pVals']['right_tail'][chunkInds[CHUNK]] = np.squeeze(groupData_across_steps[CHUNK][COND]['R_median_pVals']['right_tail'])\n",
    "            groupDataAcrossSteps[COND]['R_median_pVals']['left_tail'][chunkInds[CHUNK]] = np.squeeze(groupData_across_steps[CHUNK][COND]['R_median_pVals']['left_tail'])\n",
    "            groupDataAcrossSteps[COND]['R_median_pVals']['two_tail'][chunkInds[CHUNK]] = np.squeeze(groupData_across_steps[CHUNK][COND]['R_median_pVals']['two_tail'])\n",
    "            groupDataAcrossSteps[COND]['R_median_pVals']['map'][chunkInds[CHUNK]] = np.squeeze(groupData_across_steps[CHUNK][COND]['R_median_pVals']['map'])\n",
    "            groupDataAcrossSteps[COND]['R_mean'][chunkInds[CHUNK]] = np.squeeze(groupData_across_steps[CHUNK][COND]['R_mean'])\n",
    "            groupDataAcrossSteps[COND]['R_mean_pVals']['right_tail'][chunkInds[CHUNK]] = np.squeeze(groupData_across_steps[CHUNK][COND]['R_mean_pVals']['right_tail'])\n",
    "            groupDataAcrossSteps[COND]['R_mean_pVals']['left_tail'][chunkInds[CHUNK]] = np.squeeze(groupData_across_steps[CHUNK][COND]['R_mean_pVals']['left_tail'])\n",
    "            groupDataAcrossSteps[COND]['R_mean_pVals']['two_tail'][chunkInds[CHUNK]] = np.squeeze(groupData_across_steps[CHUNK][COND]['R_mean_pVals']['two_tail'])\n",
    "            groupDataAcrossSteps[COND]['R_mean_pVals']['map'][chunkInds[CHUNK]] = np.squeeze(groupData_across_steps[CHUNK][COND]['R_mean_pVals']['map'])\n",
    "\n",
    "            # if it's the last chunk\n",
    "            if CHUNK == numChunks-1:\n",
    "\n",
    "                # reapply FDR correction to p-values\n",
    "                groupDataAcrossSteps[COND]['R_median_pVals']['fdr_right'] = multi.fdrcorrection(groupDataAcrossSteps[COND]['R_median_pVals']['right_tail'], alpha = alpha)\n",
    "                groupDataAcrossSteps[COND]['R_median_pVals']['fdr_left'] = multi.fdrcorrection(groupDataAcrossSteps[COND]['R_median_pVals']['left_tail'], alpha = alpha)\n",
    "                groupDataAcrossSteps[COND]['R_median_pVals']['fdr_two'] = multi.fdrcorrection(groupDataAcrossSteps[COND]['R_median_pVals']['two_tail'], alpha = alpha / 2)\n",
    "                groupDataAcrossSteps[COND]['R_mean_pVals']['fdr_right'] = multi.fdrcorrection(groupDataAcrossSteps[COND]['R_mean_pVals']['right_tail'], alpha = alpha)\n",
    "                groupDataAcrossSteps[COND]['R_mean_pVals']['fdr_left'] = multi.fdrcorrection(groupDataAcrossSteps[COND]['R_mean_pVals']['left_tail'], alpha = alpha)\n",
    "                groupDataAcrossSteps[COND]['R_mean_pVals']['fdr_two'] = multi.fdrcorrection(groupDataAcrossSteps[COND]['R_mean_pVals']['two_tail'], alpha = alpha / 2)\n",
    "\n",
    "                # get R^2 masks where all voxels that did not survive FDR correction are set to zero\n",
    "                groupDataAcrossSteps[COND]['R_median_masks']['right'] = np.copy(groupDataAcrossSteps[COND]['R_median'])\n",
    "                groupDataAcrossSteps[COND]['R_median_masks']['right'][np.invert(groupDataAcrossSteps[COND]['R_median_pVals']['fdr_right'][0])] = 0\n",
    "                groupDataAcrossSteps[COND]['R_median_masks']['left'] = np.copy(groupDataAcrossSteps[COND]['R_median'])\n",
    "                groupDataAcrossSteps[COND]['R_median_masks']['left'][np.invert(groupDataAcrossSteps[COND]['R_median_pVals']['fdr_left'][0])] = 0\n",
    "                groupDataAcrossSteps[COND]['R_median_masks']['two'] = np.copy(groupDataAcrossSteps[COND]['R_median'])\n",
    "                groupDataAcrossSteps[COND]['R_median_masks']['two'][np.invert(groupDataAcrossSteps[COND]['R_median_pVals']['fdr_two'][0])] = 0\n",
    "                groupDataAcrossSteps[COND]['R_median_masks']['two_above'] = np.copy(groupDataAcrossSteps[COND]['R_median'])\n",
    "                groupDataAcrossSteps[COND]['R_median_masks']['two_above'][np.minimum(groupDataAcrossSteps[COND]['R_median_pVals']['fdr_two'][0], np.invert(groupDataAcrossSteps[COND]['R_median_pVals']['map']))] = 0\n",
    "                groupDataAcrossSteps[COND]['R_median_masks']['two_below'] = np.copy(groupDataAcrossSteps[COND]['R_median'])\n",
    "                groupDataAcrossSteps[COND]['R_median_masks']['two_below'][np.minimum(groupDataAcrossSteps[COND]['R_median_pVals']['fdr_two'][0], groupDataAcrossSteps[COND]['R_median_pVals']['map'])] = 0\n",
    "                groupDataAcrossSteps[COND]['R_mean_masks']['right'] = np.copy(groupDataAcrossSteps[COND]['R_mean'])\n",
    "                groupDataAcrossSteps[COND]['R_mean_masks']['right'][np.invert(groupDataAcrossSteps[COND]['R_mean_pVals']['fdr_right'][0])] = 0\n",
    "                groupDataAcrossSteps[COND]['R_mean_masks']['left'] = np.copy(groupDataAcrossSteps[COND]['R_mean'])\n",
    "                groupDataAcrossSteps[COND]['R_mean_masks']['left'][np.invert(groupDataAcrossSteps[COND]['R_mean_pVals']['fdr_left'][0])] = 0\n",
    "                groupDataAcrossSteps[COND]['R_mean_masks']['two'] = np.copy(groupDataAcrossSteps[COND]['R_mean'])\n",
    "                groupDataAcrossSteps[COND]['R_mean_masks']['two'][np.invert(groupDataAcrossSteps[COND]['R_mean_pVals']['fdr_two'][0])] = 0\n",
    "                groupDataAcrossSteps[COND]['R_mean_masks']['two_above'] = np.copy(groupDataAcrossSteps[COND]['R_mean'])\n",
    "                groupDataAcrossSteps[COND]['R_mean_masks']['two_above'][np.minimum(groupDataAcrossSteps[COND]['R_mean_pVals']['fdr_two'][0], np.invert(groupDataAcrossSteps[COND]['R_mean_pVals']['map']))] = 0\n",
    "                groupDataAcrossSteps[COND]['R_mean_masks']['two_below'] = np.copy(groupDataAcrossSteps[COND]['R_mean'])\n",
    "                groupDataAcrossSteps[COND]['R_mean_masks']['two_below'][np.minimum(groupDataAcrossSteps[COND]['R_mean_pVals']['fdr_two'][0], groupDataAcrossSteps[COND]['R_mean_pVals']['map'])] = 0\n",
    "\n",
    "                # get some FDR corrected p-value summary info\n",
    "                groupDataAcrossSteps[COND]['R_median_pSig']['right'] = len(np.where(groupDataAcrossSteps[COND]['R_median_pVals']['fdr_right'][0])[0]) / totalVox\n",
    "                groupDataAcrossSteps[COND]['R_median_pSig']['left'] = len(np.where(groupDataAcrossSteps[COND]['R_median_pVals']['fdr_left'][0])[0]) / totalVox\n",
    "                groupDataAcrossSteps[COND]['R_median_pSig']['two'] = len(np.where(groupDataAcrossSteps[COND]['R_median_pVals']['fdr_two'][0])[0]) / totalVox\n",
    "                groupDataAcrossSteps[COND]['R_median_pSig']['two_above'] = np.count_nonzero(np.minimum(groupDataAcrossSteps[COND]['R_median_pVals']['fdr_two'][0], np.invert(groupDataAcrossSteps[COND]['R_median_pVals']['map']))) / totalVox # proportion of voxels that show significant FDR CORRECTED R^2 values above the median\n",
    "                groupDataAcrossSteps[COND]['R_median_pSig']['two_below'] = np.count_nonzero(np.minimum(groupDataAcrossSteps[COND]['R_median_pVals']['fdr_two'][0], groupDataAcrossSteps[COND]['R_median_pVals']['map'])) / totalVox\n",
    "                groupDataAcrossSteps[COND]['R_mean_pSig']['right'] = len(np.where(groupDataAcrossSteps[COND]['R_mean_pVals']['fdr_right'][0])[0]) / totalVox\n",
    "                groupDataAcrossSteps[COND]['R_mean_pSig']['left'] = len(np.where(groupDataAcrossSteps[COND]['R_mean_pVals']['fdr_left'][0])[0]) / totalVox\n",
    "                groupDataAcrossSteps[COND]['R_mean_pSig']['two'] = len(np.where(groupDataAcrossSteps[COND]['R_mean_pVals']['fdr_two'][0])[0]) / totalVox\n",
    "                groupDataAcrossSteps[COND]['R_mean_pSig']['two_above'] = np.count_nonzero(np.minimum(groupDataAcrossSteps[COND]['R_mean_pVals']['fdr_two'][0], np.invert(groupDataAcrossSteps[COND]['R_mean_pVals']['map']))) / totalVox # proportion of voxels that show significant FDR CORRECTED R^2 values above the median\n",
    "                groupDataAcrossSteps[COND]['R_mean_pSig']['two_below'] = np.count_nonzero(np.minimum(groupDataAcrossSteps[COND]['R_mean_pVals']['fdr_two'][0], groupDataAcrossSteps[COND]['R_mean_pVals']['map'])) / totalVox\n",
    "\n",
    "                # feedback\n",
    "                print('\\nGroup data, ' + storyConds[COND] + ' condition')\n",
    "                print('% of voxels with signficant FDR-corrected MEDIAN (across pairs) mean (across steps) R^2 values: ' + str(round(groupDataAcrossSteps[COND]['R_median_pSig']['two'] * 100,2)) + '%')\n",
    "                print('% of voxels with signficant FDR-corrected MEAN (across pairs) mean (across steps) R^2 values: ' + str(round(groupDataAcrossSteps[COND]['R_mean_pSig']['two'] * 100,2)) + '%')\n",
    "\n",
    "        else:\n",
    "\n",
    "            groupDataAcrossSteps = np.nan\n",
    "\n",
    "        #################################################\n",
    "        ### group-level step-wise parameter estimates ###\n",
    "        #################################################\n",
    "\n",
    "        # preallocate step array\n",
    "        if CHUNK == 0:\n",
    "            groupDataByStep[COND] = [[]] * numSteps\n",
    "\n",
    "        # for each rolling window step...\n",
    "        for STEP in range(numSteps):\n",
    "\n",
    "            # if it's the first chunk\n",
    "            if CHUNK == 0:\n",
    "\n",
    "                # initialize structure\n",
    "                groupDataByStep[COND][STEP] = {'R_median':np.empty(totalVox),\n",
    "                                               'R_median_pVals':dict(),\n",
    "                                               'R_median_masks':dict(),\n",
    "                                               'R_median_pSig':dict(),\n",
    "                                               'R_mean':np.empty(totalVox),\n",
    "                                               'R_mean_pVals':dict(),\n",
    "                                               'R_mean_masks':dict(),\n",
    "                                               'R_mean_pSig':dict()}\n",
    "                groupDataByStep[COND][STEP]['R_median_pVals'] = {'right_tail':np.empty(totalVox),\n",
    "                                                                 'left_tail':np.empty(totalVox),\n",
    "                                                                 'two_tail':np.empty(totalVox),\n",
    "                                                                 'map':np.zeros(totalVox, dtype=bool)}\n",
    "                groupDataByStep[COND][STEP]['R_mean_pVals'] = {'right_tail':np.empty(totalVox),\n",
    "                                                               'left_tail':np.empty(totalVox),\n",
    "                                                               'two_tail':np.empty(totalVox),\n",
    "                                                               'map':np.zeros(totalVox, dtype=bool)}\n",
    "                # groupDataByStep[COND][STEP]['R_median_masks'] = {}\n",
    "                # groupDataByStep[COND][STEP]['R_mean_masks'] = {}\n",
    "\n",
    "            # get group data by step\n",
    "            groupDataByStep[COND][STEP]['R_median'][chunkInds[CHUNK]] = np.squeeze(groupData_by_step[CHUNK][COND][STEP]['R_median'])\n",
    "            groupDataByStep[COND][STEP]['R_median_pVals']['right_tail'][chunkInds[CHUNK]] = np.squeeze(groupData_by_step[CHUNK][COND][STEP]['R_median_pVals']['right_tail'])\n",
    "            groupDataByStep[COND][STEP]['R_median_pVals']['left_tail'][chunkInds[CHUNK]] = np.squeeze(groupData_by_step[CHUNK][COND][STEP]['R_median_pVals']['left_tail'])\n",
    "            groupDataByStep[COND][STEP]['R_median_pVals']['two_tail'][chunkInds[CHUNK]] = np.squeeze(groupData_by_step[CHUNK][COND][STEP]['R_median_pVals']['two_tail'])\n",
    "            groupDataByStep[COND][STEP]['R_median_pVals']['map'][chunkInds[CHUNK]] = np.squeeze(groupData_by_step[CHUNK][COND][STEP]['R_median_pVals']['map'])\n",
    "            groupDataByStep[COND][STEP]['R_mean'][chunkInds[CHUNK]] = np.squeeze(groupData_by_step[CHUNK][COND][STEP]['R_mean'])\n",
    "            groupDataByStep[COND][STEP]['R_mean_pVals']['right_tail'][chunkInds[CHUNK]] = np.squeeze(groupData_by_step[CHUNK][COND][STEP]['R_mean_pVals']['right_tail'])\n",
    "            groupDataByStep[COND][STEP]['R_mean_pVals']['left_tail'][chunkInds[CHUNK]] = np.squeeze(groupData_by_step[CHUNK][COND][STEP]['R_mean_pVals']['left_tail'])\n",
    "            groupDataByStep[COND][STEP]['R_mean_pVals']['two_tail'][chunkInds[CHUNK]] = np.squeeze(groupData_by_step[CHUNK][COND][STEP]['R_mean_pVals']['two_tail'])\n",
    "            groupDataByStep[COND][STEP]['R_mean_pVals']['map'][chunkInds[CHUNK]] = np.squeeze(groupData_by_step[CHUNK][COND][STEP]['R_mean_pVals']['map'])\n",
    "\n",
    "            # if on the last chunk...\n",
    "            if CHUNK == numChunks-1:\n",
    "\n",
    "                # reapply false discovery rate correction across all voxels\n",
    "                groupDataByStep[COND][STEP]['R_median_pVals']['fdr_right'] = multi.fdrcorrection(groupDataByStep[COND][STEP]['R_median_pVals']['right_tail'], alpha = alpha)\n",
    "                groupDataByStep[COND][STEP]['R_median_pVals']['fdr_left'] = multi.fdrcorrection(groupDataByStep[COND][STEP]['R_median_pVals']['left_tail'], alpha = alpha)\n",
    "                groupDataByStep[COND][STEP]['R_median_pVals']['fdr_two'] = multi.fdrcorrection(groupDataByStep[COND][STEP]['R_median_pVals']['two_tail'], alpha = alpha / 2)\n",
    "                groupDataByStep[COND][STEP]['R_mean_pVals']['fdr_right'] = multi.fdrcorrection(groupDataByStep[COND][STEP]['R_mean_pVals']['right_tail'], alpha = alpha)\n",
    "                groupDataByStep[COND][STEP]['R_mean_pVals']['fdr_left'] = multi.fdrcorrection(groupDataByStep[COND][STEP]['R_mean_pVals']['left_tail'], alpha = alpha)\n",
    "                groupDataByStep[COND][STEP]['R_mean_pVals']['fdr_two'] = multi.fdrcorrection(groupDataByStep[COND][STEP]['R_mean_pVals']['two_tail'], alpha = alpha / 2)\n",
    "\n",
    "                # get R^2 masks where all voxels that did not survive FDR correction are set to zero\n",
    "                groupDataByStep[COND][STEP]['R_median_masks']['right'] = np.copy(groupDataByStep[COND][STEP]['R_median'])\n",
    "                groupDataByStep[COND][STEP]['R_median_masks']['right'][np.invert(groupDataByStep[COND][STEP]['R_median_pVals']['fdr_right'][0])] = 0\n",
    "                groupDataByStep[COND][STEP]['R_median_masks']['left'] = np.copy(groupDataByStep[COND][STEP]['R_median'])\n",
    "                groupDataByStep[COND][STEP]['R_median_masks']['left'][np.invert(groupDataByStep[COND][STEP]['R_median_pVals']['fdr_left'][0])] = 0\n",
    "                groupDataByStep[COND][STEP]['R_median_masks']['two'] = np.copy(groupDataByStep[COND][STEP]['R_median'])\n",
    "                groupDataByStep[COND][STEP]['R_median_masks']['two'][np.invert(groupDataByStep[COND][STEP]['R_median_pVals']['fdr_two'][0])] = 0\n",
    "                groupDataByStep[COND][STEP]['R_median_masks']['two_above'] = np.copy(groupDataByStep[COND][STEP]['R_median'])\n",
    "                groupDataByStep[COND][STEP]['R_median_masks']['two_above'][np.minimum(groupDataByStep[COND][STEP]['R_median_pVals']['fdr_two'][0], np.invert(groupDataByStep[COND][STEP]['R_median_pVals']['map']))] = 0\n",
    "                groupDataByStep[COND][STEP]['R_median_masks']['two_below'] = np.copy(groupDataByStep[COND][STEP]['R_median'])\n",
    "                groupDataByStep[COND][STEP]['R_median_masks']['two_below'][np.minimum(groupDataByStep[COND][STEP]['R_median_pVals']['fdr_two'][0], groupDataByStep[COND][STEP]['R_median_pVals']['map'])] = 0\n",
    "                groupDataByStep[COND][STEP]['R_mean_masks']['right'] = np.copy(groupDataByStep[COND][STEP]['R_mean'])\n",
    "                groupDataByStep[COND][STEP]['R_mean_masks']['right'][np.invert(groupDataByStep[COND][STEP]['R_mean_pVals']['fdr_right'][0])] = 0\n",
    "                groupDataByStep[COND][STEP]['R_mean_masks']['left'] = np.copy(groupDataByStep[COND][STEP]['R_mean'])\n",
    "                groupDataByStep[COND][STEP]['R_mean_masks']['left'][np.invert(groupDataByStep[COND][STEP]['R_mean_pVals']['fdr_left'][0])] = 0\n",
    "                groupDataByStep[COND][STEP]['R_mean_masks']['two'] = np.copy(groupDataByStep[COND][STEP]['R_mean'])\n",
    "                groupDataByStep[COND][STEP]['R_mean_masks']['two'][np.invert(groupDataByStep[COND][STEP]['R_mean_pVals']['fdr_two'][0])] = 0\n",
    "                groupDataByStep[COND][STEP]['R_mean_masks']['two_above'] = np.copy(groupDataByStep[COND][STEP]['R_mean'])\n",
    "                groupDataByStep[COND][STEP]['R_mean_masks']['two_above'][np.minimum(groupDataByStep[COND][STEP]['R_mean_pVals']['fdr_two'][0], np.invert(groupDataByStep[COND][STEP]['R_mean_pVals']['map']))] = 0\n",
    "                groupDataByStep[COND][STEP]['R_mean_masks']['two_below'] = np.copy(groupDataByStep[COND][STEP]['R_mean'])\n",
    "                groupDataByStep[COND][STEP]['R_mean_masks']['two_below'][np.minimum(groupDataByStep[COND][STEP]['R_mean_pVals']['fdr_two'][0], groupDataByStep[COND][STEP]['R_mean_pVals']['map'])] = 0\n",
    "\n",
    "                # get some FDR corrected p-value summary info\n",
    "                groupDataByStep[COND][STEP]['R_median_pSig']['right'] = len(np.where(groupDataByStep[COND][STEP]['R_median_pVals']['fdr_right'][0])[0]) / totalVox\n",
    "                groupDataByStep[COND][STEP]['R_median_pSig']['left'] = len(np.where(groupDataByStep[COND][STEP]['R_median_pVals']['fdr_left'][0])[0]) / totalVox\n",
    "                groupDataByStep[COND][STEP]['R_median_pSig']['two'] = len(np.where(groupDataByStep[COND][STEP]['R_median_pVals']['fdr_two'][0])[0]) / totalVox\n",
    "                groupDataByStep[COND][STEP]['R_median_pSig']['two_above'] = np.count_nonzero(np.minimum(groupDataByStep[COND][STEP]['R_median_pVals']['fdr_two'][0], np.invert(groupDataByStep[COND][STEP]['R_median_pVals']['map']))) / totalVox # proportion of voxels that show significant FDR CORRECTED R^2 values above the median\n",
    "                groupDataByStep[COND][STEP]['R_median_pSig']['two_below'] = np.count_nonzero(np.minimum(groupDataByStep[COND][STEP]['R_median_pVals']['fdr_two'][0], groupDataByStep[COND][STEP]['R_median_pVals']['map'])) / totalVox\n",
    "                groupDataByStep[COND][STEP]['R_mean_pSig']['right'] = len(np.where(groupDataByStep[COND][STEP]['R_mean_pVals']['fdr_right'][0])[0]) / totalVox\n",
    "                groupDataByStep[COND][STEP]['R_mean_pSig']['left'] = len(np.where(groupDataByStep[COND][STEP]['R_mean_pVals']['fdr_left'][0])[0]) / totalVox\n",
    "                groupDataByStep[COND][STEP]['R_mean_pSig']['two'] = len(np.where(groupDataByStep[COND][STEP]['R_mean_pVals']['fdr_two'][0])[0]) / totalVox\n",
    "                groupDataByStep[COND][STEP]['R_mean_pSig']['two_above'] = np.count_nonzero(np.minimum(groupDataByStep[COND][STEP]['R_mean_pVals']['fdr_two'][0], np.invert(groupDataByStep[COND][STEP]['R_mean_pVals']['map']))) / totalVox # proportion of voxels that show significant FDR CORRECTED R^2 values above the median\n",
    "                groupDataByStep[COND][STEP]['R_mean_pSig']['two_below'] = np.count_nonzero(np.minimum(groupDataByStep[COND][STEP]['R_mean_pVals']['fdr_two'][0], groupDataByStep[COND][STEP]['R_mean_pVals']['map'])) / totalVox\n",
    "\n",
    "                # feedback\n",
    "                print('\\nGroup data, ' + storyConds[COND] + ' condition, step ' + str(STEP+1))\n",
    "                print('% of voxels with signficant FDR-corrected MEDIAN (across pairs) mean (across steps) R^2 values: ' + str(round(groupDataByStep[COND][STEP]['R_median_pSig']['two'] * 100,2)) + '%')\n",
    "                print('% of voxels with signficant FDR-corrected MEAN (across pairs) mean (across steps) R^2 values: ' + str(round(groupDataByStep[COND][STEP]['R_mean_pSig']['two'] * 100,2)) + '%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% concatenate chunked data and reapply FDR correction\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total duration: ~3.05 min\n"
     ]
    }
   ],
   "source": [
    "totalDur = round((time.time() - startTime) / 60,2) # [min]\n",
    "print('\\ntotal duration: ~' + str(totalDur) + ' min')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Get total processing time\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "duration = {'chunkDur':chunkDur,\n",
    "            'totalDur':totalDur}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Package things into dictionaries\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving output...\n",
      "output saved here: /dartfs-hpc/rc/lab/W/WheatleyT/f00589z/hyperscanning/storytelling/lag_model_output/lagModel_mode0_shuffRange0_perms10_padmean_pScram.pkl\n"
     ]
    }
   ],
   "source": [
    "if saveOutput:\n",
    "\n",
    "    # set save file name -- in .py file just save a parameters variable so you dont have to make the\n",
    "    # save name wildly long...\n",
    "    saveName = 'lagModel_mode' + str(rollMode)\n",
    "    if rollMode == 3:\n",
    "        saveName = saveName + '_winTRs' + str(winTRs) + '_stepSize' + str(stepSize) + '_numSteps' + str(numSteps)\n",
    "    saveName = saveName + '_shuffRange' + str(shuffleRange) + '_perms' + str(permuts) + '_pad' + padding\n",
    "    if circShift:\n",
    "        saveName = saveName + '_cShift'\n",
    "    else:\n",
    "        saveName = saveName + '_pScram'\n",
    "    if pseudo:\n",
    "        saveName = saveName + '_pseudo'\n",
    "    print('saving output...')\n",
    "    saveFile = outputFolder + saveName + '.pkl'\n",
    "    with open(saveFile,'wb') as f:\n",
    "            pickle.dump([rngSeed, pairMap, chunkInds, pairDataByStep, groupDataByStep, pairDataAcrossSteps, groupDataAcrossSteps, duration], f, protocol=4)\n",
    "    print('output saved here: ' + saveFile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Save output\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('\\n\\n\\nThe loony bin is closing shop, greetings',\n",
    "          'from The Combine!\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% say goodbye\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def surfaceStatMap(masker,statMapVec,avgSurface,thresh=0, conditions=['']):\n",
    "\n",
    "    \"\"\"\n",
    "    :param masker:\n",
    "    :param statMapVec:\n",
    "    :param avgSurface:\n",
    "    :param thresh:\n",
    "    :param conditions: array of strings corresponding to different conditions\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # preallocate task arrays\n",
    "    statMap = [[]] * len(conditions)\n",
    "    texture = [[]] * len(conditions)\n",
    "    view = [[]] * len(conditions)\n",
    "\n",
    "    # for each task...\n",
    "    for COND in range(len(conditions)):\n",
    "\n",
    "        # get stat map\n",
    "        statMap[COND] = masker.inverse_transform(statMapVec[COND])\n",
    "\n",
    "        # surface plot\n",
    "        texture[COND] = [[]] * 2\n",
    "        view[COND] = [[]] * 2\n",
    "\n",
    "        for HEMI in [0,1]:\n",
    "            if HEMI == 0:\n",
    "                texture[COND][HEMI] = surface.vol_to_surf(statMap[COND], avgSurface.pial_left)\n",
    "                view[COND][HEMI] = plotting.view_surf(avgSurface.infl_left,\n",
    "                                                                       texture[COND][HEMI],\n",
    "                                                                       threshold=thresh,\n",
    "                                                                       colorbar=True,\n",
    "                                                                       title= conditions[COND] + ' left',\n",
    "                                                                       bg_map=avgSurface.sulc_left)\n",
    "            else:\n",
    "                texture[COND][HEMI] = surface.vol_to_surf(statMap[COND], avgSurface.pial_right)\n",
    "                view[COND][HEMI] = plotting.view_surf(avgSurface.infl_right,\n",
    "                                                                       texture[COND][HEMI],\n",
    "                                                                       threshold=thresh,\n",
    "                                                                       colorbar=True,\n",
    "                                                                       title=conditions[COND] + ', right',\n",
    "                                                                       bg_map=avgSurface.sulc_right)\n",
    "\n",
    "    return view"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% define function for plotting surface stat maps\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get masker object and average surface mesh\n",
    "maskFile = '/dartfs-hpc/rc/home/z/f00589z/hyperscanning/control_tasks/nuisRegr_input_files/mni_asym09c_mask_resamp3x3.nii.gz'\n",
    "maskImg = nImage.load_img(maskFile)\n",
    "masker = input_data.NiftiMasker(maskImg)\n",
    "masker.fit_transform(maskImg)\n",
    "fsaverage = datasets.fetch_surf_fsaverage()\n",
    "\n",
    "# reformat data for stat map function and get joint - ind stat map vector\n",
    "statVecs = [[]] * 3\n",
    "STEP = 0\n",
    "for COND in range(len(statVecs)):\n",
    "    statVecs[COND] = groupDataAcrossSteps[COND]['R_median_masks']['two']\n",
    "\n",
    "# make stat maps\n",
    "view = surfaceStatMap(masker,statVecs,fsaverage,0.001,['independent','joint','joint - independent'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% get masker object and plot surface stat maps\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "view[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% independent left\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "view[0][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% independent right\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "view[1][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% joint left\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "view[1][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% joint right\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "view[2][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% joint - independent left\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "view[2][1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% joint - independent right\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "hypescancentral",
   "language": "python",
   "display_name": "hypeScanKernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}