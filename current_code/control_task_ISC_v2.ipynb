{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "pycharm": {
     "name": "#%% imports\n"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.multitest as multi\n",
    "sys.path.append('/afs/dbic.dartmouth.edu/usr/wheatley/jd/')\n",
    "from phaseScramble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pairNum      subID\n",
      "0         2  sid000007\n",
      "1         3  sid000009\n",
      "2         4  sid000560\n",
      "3         5  sid000535\n",
      "4         6  sid000102\n",
      "5         7  sid000416\n",
      "6         8  sid000499\n",
      "7         9  sid000142\n",
      "8         2  hid000002\n",
      "9         3  hid000003\n",
      "10        4  hid000004\n",
      "11        5  hid000005\n",
      "12        6  hid000006\n",
      "13        7  hid000007\n",
      "14        8  hid000008\n",
      "15        9  hid000009\n"
     ]
    }
   ],
   "source": [
    "# mark starting time\n",
    "startTime = time.time()\n",
    "\n",
    "# participant IDs\n",
    "dbicIDs = np.array([\"sid000007\", \"sid000009\", \"sid000560\", \"sid000535\", \"sid000102\", \"sid000416\", \"sid000499\", \"sid000142\"])\n",
    "cbsIDs = np.array([\"hid000002\", \"hid000003\", \"hid000004\", \"hid000005\", \"hid000006\", \"hid000007\", \"hid000008\", \"hid000009\"])\n",
    "\n",
    "# pair numbers\n",
    "pairNums = np.arange(2,len(dbicIDs)+2)\n",
    "\n",
    "# make subject list data frame\n",
    "subList = pd.DataFrame(np.transpose(np.tile(pairNums, (1, 2))),columns=['pairNum'])\n",
    "subList['subID'] = np.concatenate((dbicIDs, cbsIDs), axis=0)\n",
    "print(subList)\n",
    "\n",
    "# get number of participants\n",
    "numSubs = len(pairNums) * 2\n",
    "\n",
    "# set number of permutations\n",
    "permutations = 100\n",
    "\n",
    "# indicate whether or not we're debugging (if so, use a small subset of TRs and voxels to speed things up)\n",
    "debug = False\n",
    "debugTRs = 201\n",
    "debugVox = 800\n",
    "\n",
    "# set alpha for permutation tests\n",
    "alpha = 0.05\n",
    "\n",
    "# number of jobs for joblib\n",
    "numJobs = 32\n",
    "\n",
    "# set joblib verbosity -- don't go over 50 lest ye print like a million outputs and slow everything down\n",
    "verbosity = 50\n",
    "\n",
    "# select whether or not to save output\n",
    "saveOutput = False\n",
    "\n",
    "# set fitting distribution to normal\n",
    "dist = getattr(stats, 'norm')\n",
    "\n",
    "# define task names\n",
    "taskNames = np.array(['listening','reading'])\n",
    "\n",
    "# use joblib or not\n",
    "parallel = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% setup\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 487 x 69880 timeseries for listening task, sub sid000007\n",
      "loaded 487 x 69880 timeseries for listening task, sub sid000009\n",
      "loaded 487 x 69880 timeseries for listening task, sub sid000560\n",
      "loaded 487 x 69880 timeseries for listening task, sub sid000535\n",
      "loaded 487 x 69880 timeseries for listening task, sub sid000102\n",
      "loaded 487 x 69880 timeseries for listening task, sub sid000416\n",
      "loaded 487 x 69880 timeseries for listening task, sub sid000499\n",
      "loaded 487 x 69880 timeseries for listening task, sub sid000142\n",
      "loaded 487 x 69880 timeseries for listening task, sub hid000002\n",
      "loaded 487 x 69880 timeseries for listening task, sub hid000003\n",
      "loaded 487 x 69880 timeseries for listening task, sub hid000004\n",
      "loaded 487 x 69880 timeseries for listening task, sub hid000005\n",
      "loaded 487 x 69880 timeseries for listening task, sub hid000006\n",
      "loaded 487 x 69880 timeseries for listening task, sub hid000007\n",
      "loaded 487 x 69880 timeseries for listening task, sub hid000008\n",
      "loaded 487 x 69880 timeseries for listening task, sub hid000009\n",
      "loaded 236 x 69880 timeseries for reading task, sub sid000007\n",
      "loaded 236 x 69880 timeseries for reading task, sub sid000009\n",
      "loaded 236 x 69880 timeseries for reading task, sub sid000560\n",
      "loaded 236 x 69880 timeseries for reading task, sub sid000535\n",
      "loaded 236 x 69880 timeseries for reading task, sub sid000102\n",
      "loaded 236 x 69880 timeseries for reading task, sub sid000416\n",
      "loaded 236 x 69880 timeseries for reading task, sub sid000499\n",
      "loaded 236 x 69880 timeseries for reading task, sub sid000142\n",
      "loaded 236 x 69880 timeseries for reading task, sub hid000002\n",
      "loaded 236 x 69880 timeseries for reading task, sub hid000003\n",
      "loaded 236 x 69880 timeseries for reading task, sub hid000004\n",
      "loaded 236 x 69880 timeseries for reading task, sub hid000005\n",
      "loaded 236 x 69880 timeseries for reading task, sub hid000006\n",
      "loaded 236 x 69880 timeseries for reading task, sub hid000007\n",
      "loaded 236 x 69880 timeseries for reading task, sub hid000008\n",
      "loaded 236 x 69880 timeseries for reading task, sub hid000009\n"
     ]
    }
   ],
   "source": [
    "# set data folder\n",
    "folder = '/afs/dbic.dartmouth.edu/usr/wheatley/jd/control_tasks_v2/'\n",
    "\n",
    "# loop through participants...\n",
    "boldData = [[]] * 2\n",
    "for TASK in [0,1]: #for each task, listening, then reading\n",
    "\n",
    "    # preallocate task data list\n",
    "    boldData[TASK] = [[]] * numSubs\n",
    "\n",
    "    for SUB in range(numSubs):\n",
    "\n",
    "        # get file name\n",
    "        fileName = folder + 'sub-' + subList['subID'][SUB] + '_ses-pair0' + str(subList['pairNum'][SUB]) + '_task-storytelling' + str(TASK + 3) + '_run-0' + str(TASK + 3) + '_bold_space-MNI152NLin2009cAsym_preproc_nuisRegr_2021_interp.mat'\n",
    "\n",
    "        #load data\n",
    "        tmp = sio.loadmat(fileName) #load file\n",
    "        boldData[TASK][SUB] = tmp['tseries'] #get timeseries data\n",
    "        print('loaded ' + str(boldData[TASK][SUB].shape[0]) + ' x ' + str(boldData[TASK][SUB].shape[1]) + ' timeseries for ' + taskNames[TASK] + ' task, sub ' + subList['subID'][SUB])\n",
    "\n",
    "        if debug:\n",
    "\n",
    "            # take a small subset of each timeseries to speed up the debugging process\n",
    "            boldData[TASK][SUB] = boldData[TASK][SUB][np.ix_(np.arange(debugTRs),range(debugVox))]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% load listening and reading timeseries\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parallelSubWrapper(SUB,numSubs,boldData,permutations):\n",
    "    \"\"\"\n",
    "    function to use with joblib below to run permutation tests for participants in parallel\n",
    "    :param SUB: participant index\n",
    "    :param numSubs: total number of participants in the analysis\n",
    "    :param boldData: timeseries data [timepoints x voxels]\n",
    "    :param permutations: number of permutations to use\n",
    "    :return:\n",
    "        corrData: real voxelwise correlations between participant and the mean across all other participants [voxels x 1]\n",
    "        nullCorr: null voxelwise correlations between participant and the mean across all other participants [permutations x voxels]\n",
    "        permTest: array containing various results from permutation test and normal fits to the null distribution (see permTestMap variable above and comments below)\n",
    "    \"\"\"\n",
    "\n",
    "    # get mean of data from all participants EXCEPT the current participant\n",
    "    otherSubs = np.arange(0,numSubs)\n",
    "    otherSubs = np.delete(otherSubs,SUB)\n",
    "    groupMean = np.mean([boldData[i] for i in otherSubs], axis=0)\n",
    "\n",
    "    # get REAL correlation between current participant and groupMean\n",
    "    corrData = fastColumnCorr(boldData[SUB], groupMean)\n",
    "\n",
    "    # generate null correlations\n",
    "    nullCorr = [[]] * permutations\n",
    "    print(boldData[SUB].shape)\n",
    "    print(groupMean.shape)\n",
    "    for PERM in range(permutations): # for each permutation...\n",
    "        nullCorr[PERM] = fastColumnCorr(phase_scrambling(boldData[SUB]),groupMean)\n",
    "\n",
    "    # convert permutation data to a permutations x voxels array\n",
    "    nullCorr = np.asarray(nullCorr)\n",
    "\n",
    "    # preallocate sublists of permTest array\n",
    "    permTest = [[]] * len(permTestMap)\n",
    "\n",
    "    # get permutation test p-values and apply FDR correction\n",
    "    permTest[0] = [[]] * nullCorr.shape[1] # permutation test p-values\n",
    "    for VOX in range(nullCorr.shape[1]):\n",
    "        permTest[0][VOX] = len(np.where(abs(nullCorr[:,VOX]) > abs(corrData[VOX]))[0]) / float(permutations)\n",
    "    permTest[1] = multi.fdrcorrection(permTest[0], alpha=alpha) # FDR correction\n",
    "    permTest[2] = [[]] * 2 # preallocate lists for permutation test summary info\n",
    "    permTest[2][0] = len(np.where(np.array(permTest[0]) < alpha)[0]) # proportion of voxels that show significant correlations with the group\n",
    "    permTest[2][1] = len(np.where(permTest[1][0])[0]) / nullCorr.shape[1] # proportion of voxels that show significant FDR CORRECTED correlations with the group\n",
    "\n",
    "    # fit normal distributions to null data\n",
    "    permTest[3] = [[]] * nullCorr.shape[1] # preallocate list for voxelwise normal dist parameters\n",
    "    permTest[4] = [[]] * nullCorr.shape[1] # preallocate list for voxelwise Kolmogorovâ€“Smirnov test results\n",
    "    permTest[5] = np.zeros((nullCorr.shape[1],), dtype=int) # preallocate voxelwise array to indicate bad fits\n",
    "    for VOX in range(nullCorr.shape[1]): # for each voxel...\n",
    "        permTest[3][VOX] = dist.fit(nullCorr[:,VOX]) # fit normal distribution\n",
    "        permTest[4][VOX] = stats.kstest(nullCorr[:,VOX], \"norm\", permTest[3][VOX]) # measure goodness of fit\n",
    "        if permTest[4][VOX][1] < alpha: # if a voxel has a bad normal fit...\n",
    "            permTest[5][VOX] = 1 # flag it\n",
    "    permTest[6] = pd.DataFrame(data={'numBadFits': [sum(permTest[5])], 'propBadFits': [sum(permTest[5]) / len(permTest[5])]}) # number and proportion of voxels with bad normal fits to the null distribution\n",
    "\n",
    "    return corrData, nullCorr, permTest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% parallel subject loop wrapper function\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Using backend LokyBackend with 32 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# preallocate correlation lists\n",
    "corrData = [[]] * 2\n",
    "nullCorr = [[]] * 2\n",
    "permTest = [[]] * 2\n",
    "\n",
    "# map of what each participant-specific sublist in permTestMap contains\n",
    "permTestMap = ['permPval','FDR_corrected_permPval','propSigFDRvoxels','normParams','KStest','badFits','badFitsSummary']\n",
    "\n",
    "# Voxelwise correlation between participant and the rest of the group (mean)\n",
    "for TASK in [0,1]: # for each task...\n",
    "\n",
    "    corrData[TASK] = [[]] * numSubs\n",
    "    nullCorr[TASK] = [[]] * numSubs\n",
    "    permTest[TASK] = [[]] * numSubs\n",
    "\n",
    "    # run ISC\n",
    "    if parallel:\n",
    "\n",
    "        # run joblib\n",
    "        tmp = Parallel(n_jobs=numJobs, verbose=verbosity)(delayed(parallelSubWrapper)(SUB,numSubs,boldData[TASK],permutations) for SUB in range(numSubs))\n",
    "\n",
    "        # assign joblib outputs\n",
    "        for SUB in range(numSubs):\n",
    "            corrData[TASK][SUB] = tmp[SUB][0]\n",
    "            nullCorr[TASK][SUB] = tmp[SUB][1]\n",
    "            permTest[TASK][SUB] = tmp[SUB][2]\n",
    "    else:\n",
    "\n",
    "        for SUB in range(numSubs):\n",
    "            corrData[TASK][SUB], nullCorr[TASK][SUB], permTest[TASK][SUB] = parallelSubWrapper(SUB,numSubs,boldData[TASK],permutations)\n",
    "\n",
    "    # print some permutation test and goodness of fit summary info\n",
    "    print('\\nFinished processing participant ' + str(SUB + 1) + ' of ' + str(numSubs) + ' for task ' + str(TASK + 3))\n",
    "    print('% voxels with FDR corrected significant correlation with group: ' + str((permTest[TASK][SUB][2][1]) * 100) + '%')\n",
    "    print('% voxels for which null dist. was normal: ' + str((1 - permTest[TASK][SUB][6].iloc[0,1]) * 100) + '%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% permutation tests\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get group level null distributions for each voxel\n",
    "if numSubs > 1: # if we're running the analysis on more than one participant...\n",
    "\n",
    "    # reallocate lists for each task\n",
    "    groupNull = [[]] * 2\n",
    "\n",
    "    # for each task...\n",
    "    for TASK in [0,1]:\n",
    "\n",
    "        # preallocate sublists for groupNull array\n",
    "        groupNull[TASK] = [[]] * 5\n",
    "\n",
    "        # concatenate data from the first two participants to get things started\n",
    "        groupNull[TASK][0] = np.concatenate((nullCorr[TASK][0],nullCorr[TASK][1]),axis=0)\n",
    "\n",
    "        # concatenate data from any remaining participants\n",
    "        if numSubs > 2: # if we're running the analysis on more than two participants...\n",
    "            for SUB in range(2,numSubs): # for each participant...\n",
    "                 groupNull[TASK][0] = np.concatenate((groupNull[TASK][0],nullCorr[TASK][SUB]),axis=0) # concatenate\n",
    "\n",
    "        # fit normal distribution to group null\n",
    "        groupNull[TASK][1] = [[]] * groupNull[TASK][0].shape[1] # preallocate list for each voxel\n",
    "        groupNull[TASK][2] = [[]] * groupNull[TASK][0].shape[1] # preallocate list for each voxel\n",
    "        groupNull[TASK][3] = np.zeros((groupNull[TASK][0].shape[1],), dtype=int)\n",
    "        for VOX in range(groupNull[TASK][0].shape[1]): # for each voxel...\n",
    "            groupNull[TASK][1][VOX] = dist.fit(groupNull[TASK][0][:,VOX]) # fit normal distribution\n",
    "            groupNull[TASK][2][VOX] = stats.kstest(groupNull[TASK][0][:,VOX], \"norm\", groupNull[TASK][1][VOX]) # get KS goodness of fit\n",
    "            if groupNull[TASK][2][VOX][1] < alpha:\n",
    "                groupNull[TASK][3][VOX] = 1\n",
    "        groupNull[TASK][4] = pd.DataFrame(data={'numBadFits': [sum(groupNull[TASK][3])], 'propBadFits': [sum(groupNull[TASK][3]) / len(groupNull[TASK][3])]}) # number and proportion of voxels with bad normal fits to the group null distribution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% group level null distribution\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "endTime = time.time()\n",
    "duration = (endTime - startTime) / 60 #[min]\n",
    "print('control tasks ISC duration: ' + str(duration))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Get total analysis duration\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if saveOutput:\n",
    "    import pickle\n",
    "    with open(folder + 'controlISC.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump([permTest, corrData, groupNull, duration], f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% save data\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # what load syntax should look like:\n",
    "# loadFile = folder + 'controlISC.pkl'\n",
    "# with open(loadFile, 'rb') as f:\n",
    "#     permTest, corrData, groupNull, duration = pickle.load(f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}