{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import scipy.io as sio\n",
    "# import os\n",
    "import sys\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "# import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.multitest as multi\n",
    "sys.path.append('/afs/dbic.dartmouth.edu/usr/wheatley/jd/')\n",
    "from phaseScramble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pairNum      subID\n",
      "0         2  sid000007\n",
      "1         3  sid000009\n",
      "2         4  sid000560\n",
      "3         5  sid000535\n",
      "4         6  sid000102\n",
      "5         7  sid000416\n",
      "6         8  sid000499\n",
      "7         9  sid000142\n",
      "8         2  hid000002\n",
      "9         3  hid000003\n",
      "10        4  hid000004\n",
      "11        5  hid000005\n",
      "12        6  hid000006\n",
      "13        7  hid000007\n",
      "14        8  hid000008\n",
      "15        9  hid000009\n"
     ]
    }
   ],
   "source": [
    "#mark starting time\n",
    "startTime = time.time()\n",
    "\n",
    "#participant IDs\n",
    "dbicIDs = np.array([\"sid000007\", \"sid000009\", \"sid000560\", \"sid000535\", \"sid000102\", \"sid000416\", \"sid000499\", \"sid000142\"])\n",
    "cbsIDs = np.array([\"hid000002\", \"hid000003\", \"hid000004\", \"hid000005\", \"hid000006\", \"hid000007\", \"hid000008\", \"hid000009\"])\n",
    "\n",
    "#pair numbers\n",
    "pairNums = np.arange(2,len(dbicIDs)+2)\n",
    "\n",
    "#make subject list data frame\n",
    "subList = pd.DataFrame(np.transpose(np.tile(pairNums, (1, 2))),columns=['pairNum'])\n",
    "subList['subID'] = np.concatenate((dbicIDs, cbsIDs), axis=0)\n",
    "print(subList)\n",
    "\n",
    "#get number of participants\n",
    "numSubs = len(pairNums) * 2\n",
    "\n",
    "#set number of permutations\n",
    "permutations = 1000\n",
    "\n",
    "#indicate whether or not we're debugging (if so, use a small subset of TRs and voxels to speed things up)\n",
    "debug = True\n",
    "debugTRs = 400\n",
    "debugVox = 800\n",
    "\n",
    "#set alpha for permutation tests\n",
    "alpha = 0.05\n",
    "\n",
    "#number of jobs for joblib\n",
    "numJobs = 16\n",
    "\n",
    "#set joblib verbosity -- don't go over 50 lest ye print like a million outputs and slow everything down\n",
    "verbosity = 50\n",
    "\n",
    "#select whether or not to save output\n",
    "saveOutput = False\n",
    "\n",
    "#set fitting distribution to normal\n",
    "dist = getattr(stats, 'norm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ### setup ###\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#set data folder\n",
    "folder = '/afs/dbic.dartmouth.edu/usr/wheatley/jd/control_tasks/'\n",
    "\n",
    "#loop through participants...\n",
    "boldData = [[]] * 2\n",
    "dataShape = [[]] * 2 #******PART OF HACK! REMOVE LATER!*******\n",
    "for TASK in [0,1]: #for each task... reading, then listening *********CHECK THIS!!!!***********\n",
    "\n",
    "    #preallocate task data list\n",
    "    boldData[TASK] = [[]] * numSubs\n",
    "    dataShape[TASK] = [[]] * numSubs #******PART OF HACK! REMOVE LATER!*******\n",
    "\n",
    "    for SUB in range(numSubs):\n",
    "\n",
    "        #get file name\n",
    "        fileName = folder + 'sub-' + subList['subID'][SUB] + '_ses-pair0' + str(subList['pairNum'][SUB]) + '_task-storytelling' + str(TASK + 3) + '_run-0' + str(TASK + 3) + '_bold_space-MNI152NLin2009cAsym_preproc_nuisRegr_2021.mat'\n",
    "\n",
    "        #load data\n",
    "        tmp = sio.loadmat(fileName) #load file\n",
    "        boldData[TASK][SUB] = tmp['tseries'] #get timeseries data\n",
    "        dataShape[TASK][SUB] = boldData[TASK][SUB].shape #******PART OF HACK! REMOVE LATER!*******\n",
    "\n",
    "    #convert shape lists into dataframes and get minimum values for TRs and voxels across participants\n",
    "    dataShape[TASK] = pd.DataFrame(dataShape[TASK], columns=['TRs','voxels'])\n",
    "\n",
    "    #MAJORLY TEMPORARY HACK to deal with current differences in timeseries sizes\n",
    "    #use smallest dimensions across participants (removing rows and columns from individual time series to get there)\n",
    "    for SUB in range(numSubs):\n",
    "\n",
    "        if debug:\n",
    "\n",
    "            #take a small subset of each timeseries to speed up the debugging process\n",
    "            boldData[TASK][SUB] = boldData[TASK][SUB][np.ix_(np.arange(debugTRs),range(debugVox))]\n",
    "\n",
    "        else:\n",
    "\n",
    "            #remove \"excess\" rows and columns\n",
    "            for DIM in [0,1]: #TRs, voxels\n",
    "                if dataShape[TASK].iloc[SUB,DIM] > dataShape[TASK].iloc[:,DIM].min():\n",
    "                    numExtra = dataShape[TASK].iloc[SUB,DIM] - dataShape[TASK].iloc[:,DIM].min()\n",
    "                    toRemove = np.array(range(dataShape[TASK].iloc[SUB,DIM]-numExtra,dataShape[TASK].iloc[SUB,DIM]))\n",
    "                    boldData[TASK][SUB] = np.delete(boldData[TASK][SUB],toRemove,DIM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ### load listening and reading timeseries ###\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   1 tasks      | elapsed:   54.4s\n",
      "[Parallel(n_jobs=16)]: Done   2 out of  16 | elapsed:   54.8s remaining:  6.4min\n",
      "[Parallel(n_jobs=16)]: Done   3 out of  16 | elapsed:   57.5s remaining:  4.2min\n",
      "[Parallel(n_jobs=16)]: Done   4 out of  16 | elapsed:   57.7s remaining:  2.9min\n",
      "[Parallel(n_jobs=16)]: Done   5 out of  16 | elapsed:   57.8s remaining:  2.1min\n",
      "[Parallel(n_jobs=16)]: Done   6 out of  16 | elapsed:   57.8s remaining:  1.6min\n",
      "[Parallel(n_jobs=16)]: Done   7 out of  16 | elapsed:   58.2s remaining:  1.2min\n",
      "[Parallel(n_jobs=16)]: Done   8 out of  16 | elapsed:   58.2s remaining:   58.2s\n",
      "[Parallel(n_jobs=16)]: Done   9 out of  16 | elapsed:   58.4s remaining:   45.4s\n",
      "[Parallel(n_jobs=16)]: Done  10 out of  16 | elapsed:   58.6s remaining:   35.2s\n",
      "[Parallel(n_jobs=16)]: Done  11 out of  16 | elapsed:   58.8s remaining:   26.7s\n",
      "[Parallel(n_jobs=16)]: Done  12 out of  16 | elapsed:   58.8s remaining:   19.6s\n",
      "[Parallel(n_jobs=16)]: Done  13 out of  16 | elapsed:   59.1s remaining:   13.6s\n",
      "[Parallel(n_jobs=16)]: Done  14 out of  16 | elapsed:   59.1s remaining:    8.4s\n",
      "[Parallel(n_jobs=16)]: Done  16 out of  16 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=16)]: Done  16 out of  16 | elapsed:  1.0min remaining:    0.0s\n",
      "\n",
      "Finished processing participant 1 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 2 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 3 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 1.125%\n",
      "% voxels for which null dist. was normal: 99.375%\n",
      "\n",
      "Finished processing participant 4 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 99.375%\n",
      "\n",
      "Finished processing participant 5 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 98.875%\n",
      "\n",
      "Finished processing participant 6 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 16.625%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 7 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 8 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 9 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 46.0%\n",
      "% voxels for which null dist. was normal: 48.0%\n",
      "\n",
      "Finished processing participant 10 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 99.875%\n",
      "\n",
      "Finished processing participant 11 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 12 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 80.25%\n",
      "% voxels for which null dist. was normal: 60.375%\n",
      "\n",
      "Finished processing participant 13 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 14 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 15 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 91.5%\n",
      "% voxels for which null dist. was normal: 97.125%\n",
      "\n",
      "Finished processing participant 16 of 16 for task 3\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   1 tasks      | elapsed:   50.0s\n",
      "[Parallel(n_jobs=16)]: Done   2 out of  16 | elapsed:   50.1s remaining:  5.8min\n",
      "[Parallel(n_jobs=16)]: Done   3 out of  16 | elapsed:   51.4s remaining:  3.7min\n",
      "[Parallel(n_jobs=16)]: Done   4 out of  16 | elapsed:   51.4s remaining:  2.6min\n",
      "[Parallel(n_jobs=16)]: Done   5 out of  16 | elapsed:   51.6s remaining:  1.9min\n",
      "[Parallel(n_jobs=16)]: Done   6 out of  16 | elapsed:   51.8s remaining:  1.4min\n",
      "[Parallel(n_jobs=16)]: Done   7 out of  16 | elapsed:   52.0s remaining:  1.1min\n",
      "[Parallel(n_jobs=16)]: Done   8 out of  16 | elapsed:   52.3s remaining:   52.3s\n",
      "[Parallel(n_jobs=16)]: Done   9 out of  16 | elapsed:   52.3s remaining:   40.7s\n",
      "[Parallel(n_jobs=16)]: Done  10 out of  16 | elapsed:   52.6s remaining:   31.6s\n",
      "[Parallel(n_jobs=16)]: Done  11 out of  16 | elapsed:   52.7s remaining:   23.9s\n",
      "[Parallel(n_jobs=16)]: Done  12 out of  16 | elapsed:   52.8s remaining:   17.6s\n",
      "[Parallel(n_jobs=16)]: Done  13 out of  16 | elapsed:   53.1s remaining:   12.3s\n",
      "[Parallel(n_jobs=16)]: Done  14 out of  16 | elapsed:   53.2s remaining:    7.6s\n",
      "[Parallel(n_jobs=16)]: Done  16 out of  16 | elapsed:   53.4s finished\n",
      "[Parallel(n_jobs=16)]: Done  16 out of  16 | elapsed:   53.4s remaining:    0.0s\n",
      "\n",
      "Finished processing participant 1 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 2 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 3 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 87.25%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 4 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 5 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 6 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 7 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 32.0%\n",
      "% voxels for which null dist. was normal: 99.875%\n",
      "\n",
      "Finished processing participant 8 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 97.25%\n",
      "\n",
      "Finished processing participant 9 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 0.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 10 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 81.25%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 11 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 35.0%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 12 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 2.5%\n",
      "% voxels for which null dist. was normal: 99.875%\n",
      "\n",
      "Finished processing participant 13 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 16.875%\n",
      "% voxels for which null dist. was normal: 99.375%\n",
      "\n",
      "Finished processing participant 14 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 2.625%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 15 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 0.125%\n",
      "% voxels for which null dist. was normal: 100.0%\n",
      "\n",
      "Finished processing participant 16 of 16 for task 4\n",
      "% voxels with FDR corrected significant correlation with group: 80.875%\n",
      "% voxels for which null dist. was normal: 100.0%\n"
     ]
    }
   ],
   "source": [
    "#preallocate correlation lists\n",
    "corrData = [[]] * 2\n",
    "nullCorr = [[]] * 2\n",
    "permTest = [[]] * 2\n",
    "\n",
    "#map of what each participant-specific sublist in permTestMap contains\n",
    "permTestMap = ['permPval','FDR_corrected_permPval','propSigFDRvoxels','normParams','KStest','badFits','badFitsSummary']\n",
    "\n",
    "#Voxelwise correlation between participant and the rest of the group (mean)\n",
    "for TASK in [0,1]: #for each task...\n",
    "\n",
    "    corrData[TASK] = [[]] * numSubs\n",
    "    nullCorr[TASK] = [[]] * numSubs\n",
    "    permTest[TASK] = [[]] * numSubs\n",
    "\n",
    "    def parallelSubWrapper(SUB,numSubs,boldData,permutations):\n",
    "        \"\"\"\n",
    "        function to use with joblib below to run permutation tests for participants in parallel\n",
    "        :param SUB: participant index\n",
    "        :param numSubs: total number of participants in the analysis\n",
    "        :param boldData: timeseries data\n",
    "        :param permutations: number of permutations to use\n",
    "        :return:\n",
    "            corrData: real voxelwise correlations between participant and the mean across all other participants [voxels x 1]\n",
    "            nullCorr: null voxelwise correlations between participant and the mean across all other participants [permutations x voxels]\n",
    "            permTest: array containing various results from permutation test and normal fits to the null distribution (see permTestMap variable above and comments below)\n",
    "        \"\"\"\n",
    "\n",
    "        #get mean of data from all participants EXCEPT the current participant\n",
    "        otherSubs = np.arange(0,numSubs)\n",
    "        otherSubs = np.delete(otherSubs,SUB)\n",
    "        groupMean = np.mean([boldData[i] for i in otherSubs], axis=0)\n",
    "\n",
    "        #get REAL correlation between current participant and groupMean\n",
    "        corrData = fastColumnCorr(boldData[SUB], groupMean)\n",
    "\n",
    "        #generate null correlations\n",
    "        nullCorr = [[]] * permutations\n",
    "        for PERM in range(permutations): #for each permutation...\n",
    "            nullCorr[PERM] = fastColumnCorr(phase_scrambling(boldData[SUB]),groupMean)\n",
    "\n",
    "        #convert permutation data to a permutations x voxels array\n",
    "        nullCorr = np.asarray(nullCorr)\n",
    "\n",
    "        #preallocate sublists of permTest array\n",
    "        permTest = [[]] * len(permTestMap)\n",
    "\n",
    "        #get permutation test p-values and apply FDR correction\n",
    "        permTest[0] = [[]] * nullCorr.shape[1] #permutation test p-values\n",
    "        for VOX in range(nullCorr.shape[1]):\n",
    "            permTest[0][VOX] = len(np.where(abs(nullCorr[:,VOX]) > abs(corrData[VOX]))[0]) / float(permutations)\n",
    "        permTest[1] = multi.fdrcorrection(permTest[0], alpha=alpha) #FDR correction\n",
    "        permTest[2] = [[]] * 2 #preallocate lists for permutation test summary info\n",
    "        permTest[2][0] = len(np.where(np.array(permTest[0]) < alpha)[0]) #proportion of voxels that show significant correlations with the group\n",
    "        permTest[2][1] = len(np.where(permTest[1][0])[0]) / nullCorr.shape[1] #proportion of voxels that show significant FDR CORRECTED correlations with the group\n",
    "\n",
    "        #fit normal distributions to null data\n",
    "        permTest[3] = [[]] * nullCorr.shape[1] #preallocate list for voxelwise normal dist parameters\n",
    "        permTest[4] = [[]] * nullCorr.shape[1] #preallocate list for voxelwise Kolmogorovâ€“Smirnov test results\n",
    "        permTest[5] = np.zeros((nullCorr.shape[1],), dtype=int) #preallocate voxelwise array to indicate bad fits\n",
    "        for VOX in range(nullCorr.shape[1]): #for each voxel...\n",
    "            permTest[3][VOX] = dist.fit(nullCorr[:,VOX]) #fit normal distribution\n",
    "            permTest[4][VOX] = stats.kstest(nullCorr[:,VOX], \"norm\", permTest[3][VOX]) #measure goodness of fit\n",
    "            if permTest[4][VOX][1] < alpha: #if a voxel has a bad normal fit...\n",
    "                permTest[5][VOX] = 1 #flag it\n",
    "        permTest[6] = pd.DataFrame(data={'numBadFits': [sum(permTest[5])], 'propBadFits': [sum(permTest[5]) / len(permTest[5])]}) #number and proportion of voxels with bad normal fits to the null distribution\n",
    "\n",
    "        return corrData, nullCorr, permTest\n",
    "\n",
    "    #run joblib\n",
    "    tmp = Parallel(n_jobs=numJobs, verbose=verbosity)(delayed(parallelSubWrapper)(SUB,numSubs,boldData[TASK],permutations) for SUB in range(numSubs))\n",
    "\n",
    "    #assign joblib outputs\n",
    "    for SUB in range(numSubs):\n",
    "        corrData[TASK][SUB] = tmp[SUB][0]\n",
    "        nullCorr[TASK][SUB] = tmp[SUB][1]\n",
    "        permTest[TASK][SUB] = tmp[SUB][2]\n",
    "\n",
    "        #print some permutation test and goodness of fit summary info\n",
    "        print('\\nFinished processing participant ' + str(SUB + 1) + ' of ' + str(numSubs) + ' for task ' + str(TASK + 3))\n",
    "        print('% voxels with FDR corrected significant correlation with group: ' + str((permTest[TASK][SUB][2][1]) * 100) + '%')\n",
    "        print('% voxels for which null dist. was normal: ' + str((1 - permTest[TASK][SUB][6].iloc[0,1]) * 100) + '%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ### permutation tests ###\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#get group level null distributions for each voxel\n",
    "if numSubs > 1: #if we're running the analysis on more than one participant...\n",
    "\n",
    "    #preallocate lists for each task\n",
    "    groupNull = [[]] * 2\n",
    "\n",
    "    #for each task...\n",
    "    for TASK in [0,1]:\n",
    "\n",
    "        #preallocate sublists for groupNull array\n",
    "        #groupNull[TASK][0] = raw null data concatenated across subs [permutations*numSubs x voxels]\n",
    "        #groupNull[TASK][1] = fitted normal distribution parameters\n",
    "        #groupNull[TASK][2] = Kolmogorov-Smirnov goodness of fit test results\n",
    "        #groupNull[TASK][3] = indices of voxels with bad normal fits (KS test p-value < alpha)\n",
    "        #groupNull[TASK][4] = number and proportion of voxels with bad normal fits to the group null distribution\n",
    "        groupNull[TASK] = [[]] * 5\n",
    "\n",
    "        #concatenate data from the first two participants to get things started\n",
    "        groupNull[TASK][0] = np.concatenate((nullCorr[TASK][0],nullCorr[TASK][1]),axis=0)\n",
    "\n",
    "        #concatenate data from any remaining participants\n",
    "        if numSubs > 2: #if we're running the analysis on more than two participants...\n",
    "            for SUB in range(2,numSubs): #for each participant...\n",
    "                 groupNull[TASK][0] = np.concatenate((groupNull[TASK][0],nullCorr[TASK][SUB]),axis=0) #concatenate\n",
    "\n",
    "        #fit normal distribution to group null\n",
    "        groupNull[TASK][1] = [[]] * groupNull[TASK][0].shape[1] #preallocate list for each voxel\n",
    "        groupNull[TASK][2] = [[]] * groupNull[TASK][0].shape[1] #preallocate list for each voxel\n",
    "        groupNull[TASK][3] = np.zeros((groupNull[TASK][0].shape[1],), dtype=int)\n",
    "        for VOX in range(groupNull[TASK][0].shape[1]): #for each voxel...\n",
    "            groupNull[TASK][1][VOX] = dist.fit(groupNull[TASK][0][:,VOX]) #fit normal distribution\n",
    "            groupNull[TASK][2][VOX] = stats.kstest(groupNull[TASK][0][:,VOX], \"norm\", groupNull[TASK][1][VOX]) #get KS goodness of fit\n",
    "            if groupNull[TASK][2][VOX][1] < alpha:\n",
    "                groupNull[TASK][3][VOX] = 1\n",
    "        groupNull[TASK][4] = pd.DataFrame(data={'numBadFits': [sum(groupNull[TASK][3])], 'propBadFits': [sum(groupNull[TASK][3]) / len(groupNull[TASK][3])]}) #number and proportion of voxels with bad normal fits to the group null distribution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ### group level null distribution ###\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control tasks ISC duration: 2.129042474428813\n"
     ]
    }
   ],
   "source": [
    "endTime = time.time()\n",
    "duration = (endTime - startTime) / 60 #[min]\n",
    "print('control tasks ISC duration: ' + str(duration))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ### Get total analysis duration ###\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "if saveOutput:\n",
    "    import pickle\n",
    "    with open(folder + 'controlISC.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump([permTest, corrData, groupNull, duration], f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ### save data ###\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# #what load syntax should look like:\n",
    "# loadFile = folder + 'controlISC.pkl'\n",
    "# with open(loadFile, 'rb') as f:\n",
    "#     permTest, corrData, groupNull, duration = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# # Plot the PDF.\n",
    "# mu = permTest[TASK][SUB][0][0]\n",
    "# std = permTest[TASK][SUB][0][1]\n",
    "# xmin, xmax = plt.xlim()\n",
    "# x = np.linspace(xmin, xmax, 100)\n",
    "# p = stats.norm.pdf(x, mu, std)\n",
    "# plt.plot(x, p, 'k', linewidth=2)\n",
    "# title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n",
    "# plt.title(title)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}