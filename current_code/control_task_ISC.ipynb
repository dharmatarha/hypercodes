{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "[ADD DESCRIPTION]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.multitest as multi\n",
    "sys.path.append('/afs/dbic.dartmouth.edu/usr/wheatley/jd/')\n",
    "from phaseScramble import *\n",
    "from CircleShift import *\n",
    "from scipy.stats import norm\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% imports\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mark starting time\n",
    "startTime = time.time()\n",
    "\n",
    "# participant IDs\n",
    "dbicIDs = np.array([\"sid000007\", \"sid000009\", \"sid000560\", \"sid000535\", \"sid000102\", \"sid000416\", \"sid000499\", \"sid000142\"])\n",
    "cbsIDs = np.array([\"hid000002\", \"hid000003\", \"hid000004\", \"hid000005\", \"hid000006\", \"hid000007\", \"hid000008\", \"hid000009\"])\n",
    "\n",
    "# pair numbers\n",
    "pairNums = np.arange(2,len(dbicIDs)+2)\n",
    "\n",
    "# make subject list data frame\n",
    "subList = pd.DataFrame(np.transpose(np.tile(pairNums, (1, 2))),columns=['pairNum'])\n",
    "subList['subID'] = np.concatenate((dbicIDs, cbsIDs), axis=0)\n",
    "print(subList)\n",
    "\n",
    "# get number of participants\n",
    "numSubs = len(pairNums) * 2\n",
    "\n",
    "# set number of permutations\n",
    "permutations = 100\n",
    "\n",
    "# indicate whether or not we're debugging (if so, use a small subset of TRs and voxels to speed things up)\n",
    "debug = False\n",
    "debugTRs = 201\n",
    "debugVox = 800\n",
    "\n",
    "# set alpha for permutation tests\n",
    "alpha = 0.05\n",
    "\n",
    "# number of jobs for joblib\n",
    "numJobs = 32\n",
    "\n",
    "# set joblib verbosity -- don't go over 50 lest ye print like a million outputs and slow everything down\n",
    "verbosity = 50\n",
    "\n",
    "# select whether or not to save output\n",
    "saveOutput = True\n",
    "\n",
    "# set fitting distribution to normal\n",
    "dist = getattr(stats, 'norm')\n",
    "\n",
    "# define task names\n",
    "taskNames = np.array(['listening','reading'])\n",
    "\n",
    "# use joblib or not\n",
    "parallel = True\n",
    "\n",
    "# select whether or not to use the circle shifting method\n",
    "circShift = True\n",
    "\n",
    "# indicate whether or not to pop out some example plots of good vs bad null distribution fits\n",
    "examplePlots = False\n",
    "\n",
    "# select whether or not to normalize timeseries before ISC\n",
    "normalize = True\n",
    "\n",
    "# initialize the saveTag variable\n",
    "saveTag = ''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% setup\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set data folder\n",
    "folder = '/afs/dbic.dartmouth.edu/usr/wheatley/jd/control_tasks/'\n",
    "\n",
    "# loop through participants...\n",
    "boldData = [[]] * 2\n",
    "for TASK in [0,1]: #for each task, listening, then reading\n",
    "\n",
    "    # preallocate task data list\n",
    "    boldData[TASK] = [[]] * numSubs\n",
    "\n",
    "    for SUB in range(numSubs):\n",
    "\n",
    "        # get file name\n",
    "        fileName = folder + 'sub-' + subList['subID'][SUB] + '_ses-pair0' + str(subList['pairNum'][SUB]) + '_task-storytelling' + str(TASK + 3) + '_run-0' + str(TASK + 3) + '_bold_space-MNI152NLin2009cAsym_preproc_nuisRegr_2021_interp.mat'\n",
    "\n",
    "        #load data\n",
    "        tmp = sio.loadmat(fileName) #load file\n",
    "        boldData[TASK][SUB] = tmp['tseries'] #get timeseries data\n",
    "        print('loaded ' + str(boldData[TASK][SUB].shape[0]) + ' x ' + str(boldData[TASK][SUB].shape[1]) + ' timeseries for ' + taskNames[TASK] + ' task, sub ' + subList['subID'][SUB])\n",
    "\n",
    "        if normalize:\n",
    "            boldData[TASK][SUB] = preprocessing.normalize(boldData[TASK][SUB])\n",
    "            print('normalizing timeseries')\n",
    "            saveTag = 'norm_'\n",
    "\n",
    "        if debug:\n",
    "\n",
    "            # take a small subset of each timeseries to speed up the debugging process\n",
    "            boldData[TASK][SUB] = boldData[TASK][SUB][np.ix_(np.arange(debugTRs),range(debugVox))]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% load listening and reading timeseries\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parallelSubWrapper(SUB,numSubs,boldData,permutations,circShift):\n",
    "    \"\"\"\n",
    "    function to use with joblib below to run permutation tests for participants in parallel\n",
    "    :param SUB: participant index\n",
    "    :param numSubs: total number of participants in the analysis\n",
    "    :param boldData: timeseries data [timepoints x voxels]\n",
    "    :param permutations: number of permutations to use\n",
    "    :param circShift: boolean indicating whether or not (True or False, respectively) to use the circle shift method to \"scramble\" timeseries\n",
    "    :return:\n",
    "        corrData: real voxelwise correlations between participant and the mean across all other participants [voxels x 1]\n",
    "        nullCorr: null voxelwise correlations between participant and the mean across all other participants [permutations x voxels]\n",
    "        permTest: array containing various results from permutation test and normal fits to the null distribution (see permTestMap variable above and comments below)\n",
    "            structure of permTest:\n",
    "            permTest[0]: permutation test p-values [voxels x 1]\n",
    "            permTest[1]: FDR corrected permutation test p-values [voxels x 1]\n",
    "            permTest[2]: p-value summary stats\n",
    "                permTest[2][0]: number of voxels with uncorrected p-vals < alpha\n",
    "                permTest[2][1]: proportion of voxels with uncorrected p-vals < alpha\n",
    "                permTest[2][2]: number of voxels with FDR corrected p-vals < alpha\n",
    "                permTest[2][3]: proportion of voxels with FDR corrected p-vals < alpha\n",
    "            permTest[3]: null dist normal fit parameters [voxels x 1]\n",
    "                permTest[3][voxel][0]: mean\n",
    "                permTest[3][voxel][1]: standard deviation\n",
    "            permTest[4]: Kolmogorov–Smirnov goodness of fit test results [voxels x 1]\n",
    "                permTest[4][voxel][0]: KS test statistic\n",
    "                permTest[4][voxel][1]: p-value (values less than alpha suggest poor fit)\n",
    "            permTest[5]: logical vector for KS goodness of fit where 0 = good fit, 1 = bad fit [voxels x 1]\n",
    "            permTest[6]: table indicating number and proportion of bad normal fits to the perm-based null distribution [1 x 2 table]\n",
    "    \"\"\"\n",
    "\n",
    "    # get mean of data from all participants EXCEPT the current participant\n",
    "    otherSubs = np.arange(0,numSubs)\n",
    "    otherSubs = np.delete(otherSubs,SUB)\n",
    "    groupMean = np.mean([boldData[i] for i in otherSubs], axis=0)\n",
    "\n",
    "    # get REAL correlation between current participant and groupMean\n",
    "    corrData = fastColumnCorr(boldData[SUB], groupMean)\n",
    "\n",
    "    # generate null correlations\n",
    "    nullCorr = [[]] * permutations\n",
    "    feedbackMultiple = round(permutations / 10) # get number to use as a flag to generate feedback roughly 10 times while null correlations are being generated\n",
    "    for PERM in range(permutations): # for each permutation...\n",
    "        if circShift:\n",
    "            nullCorr[PERM] = fastColumnCorr(Circle_Shift(boldData[SUB]),groupMean)\n",
    "        else:\n",
    "            nullCorr[PERM] = fastColumnCorr(phase_scrambling(boldData[SUB]),groupMean)\n",
    "        if PERM % feedbackMultiple == 0: # if the current permutation is zero or a multiple of feedbackMultiple\n",
    "            print('getting null correlation for SUB ' + str(SUB + 1) + ', permutation ' + str(PERM + 1)) #provide feedback about which subject and which permutation we're on\n",
    "\n",
    "    # convert permutation data to a permutations x voxels array\n",
    "    nullCorr = np.asarray(nullCorr)\n",
    "\n",
    "    # preallocate sublists of permTest array\n",
    "    permTest = [[]] * len(permTestMap)\n",
    "\n",
    "    # get permutation test p-values and apply FDR correction\n",
    "    permTest[0] = [[]] * nullCorr.shape[1] # permutation test p-values\n",
    "    for VOX in range(nullCorr.shape[1]): # for each voxel...\n",
    "        permTest[0][VOX] = len(np.where(abs(nullCorr[:,VOX]) > abs(corrData[VOX]))[0]) / float(permutations) # proportion of permutations where absolute value of null correlation is greater than absolute value of real correlation\n",
    "    permTest[1] = multi.fdrcorrection(permTest[0], alpha = alpha / 2) # FDR correction - logical vector with length = # voxels\n",
    "    permTest[2] = [[]] * 4 # preallocate lists for permutation test summary info\n",
    "    permTest[2][0] = len(np.where(np.array(permTest[0]) < alpha / 2)[0]) # number of voxels that show significant correlations with the group\n",
    "    permTest[2][1] = permTest[2][0] / nullCorr.shape[1] # proportion of voxels that show significant correlations with the group\n",
    "    permTest[2][2] = np.count_nonzero(permTest[1][0]) # number of voxels that show significant FDR CORRECTED correlations with the group\n",
    "    permTest[2][3] = len(np.where(permTest[1][0])[0]) / nullCorr.shape[1] # proportion of voxels that show significant FDR CORRECTED correlations with the group\n",
    "\n",
    "    # fit normal distributions to null data\n",
    "    permTest[3] = [[]] * nullCorr.shape[1] # preallocate list for voxelwise normal dist parameters\n",
    "    permTest[4] = [[]] * nullCorr.shape[1] # preallocate list for voxelwise Kolmogorov–Smirnov test results\n",
    "    permTest[5] = np.zeros((nullCorr.shape[1],), dtype=int) # preallocate voxelwise array to indicate bad fits\n",
    "    for VOX in range(nullCorr.shape[1]): # for each voxel...\n",
    "        permTest[3][VOX] = dist.fit(nullCorr[:,VOX]) # fit normal distribution\n",
    "        permTest[4][VOX] = stats.kstest(nullCorr[:,VOX], \"norm\", permTest[3][VOX]) # measure goodness of fit\n",
    "        if permTest[4][VOX][1] < alpha: # if a voxel has a bad normal fit...\n",
    "            permTest[5][VOX] = 1 # flag it\n",
    "    permTest[6] = pd.DataFrame(data={'numBadFits': [sum(permTest[5])], 'propBadFits': [sum(permTest[5]) / len(permTest[5])]}) # number and proportion of voxels with bad normal fits to the null distribution\n",
    "\n",
    "    return corrData, nullCorr, permTest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% parallel subject loop wrapper function\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preallocate correlation lists\n",
    "corrData = [[]] * 2\n",
    "nullCorr = [[]] * 2\n",
    "permTest = [[]] * 2\n",
    "\n",
    "# map of what each participant-specific sublist in permTestMap contains\n",
    "permTestMap = ['permPval','FDR_corrected_permPval','propSigFDRvoxels','normParams','KStest','badFits','badFitsSummary']\n",
    "\n",
    "# Voxelwise correlation between participant and the rest of the group (mean)\n",
    "for TASK in [0,1]: # for each task...\n",
    "\n",
    "    corrData[TASK] = [[]] * numSubs\n",
    "    nullCorr[TASK] = [[]] * numSubs\n",
    "    permTest[TASK] = [[]] * numSubs\n",
    "\n",
    "    # run ISC\n",
    "    if parallel:\n",
    "\n",
    "        # run joblib\n",
    "        tmp = Parallel(n_jobs=numJobs, verbose=verbosity)(delayed(parallelSubWrapper)(SUB,numSubs,boldData[TASK],permutations,circShift) for SUB in range(numSubs))\n",
    "\n",
    "        # assign joblib outputs\n",
    "        for SUB in range(numSubs):\n",
    "            corrData[TASK][SUB] = tmp[SUB][0]\n",
    "            nullCorr[TASK][SUB] = tmp[SUB][1]\n",
    "            permTest[TASK][SUB] = tmp[SUB][2]\n",
    "    else:\n",
    "\n",
    "        for SUB in range(numSubs):\n",
    "            corrData[TASK][SUB], nullCorr[TASK][SUB], permTest[TASK][SUB] = parallelSubWrapper(SUB,numSubs,boldData[TASK],permutations,circShift)\n",
    "\n",
    "    # print some permutation test and goodness of fit summary info\n",
    "    print('\\nFinished processing participant ' + str(SUB + 1) + ' of ' + str(numSubs) + ' for task ' + str(TASK + 3))\n",
    "    print('% voxels with FDR corrected significant correlation with group: ' + str((permTest[TASK][SUB][2][1]) * 100) + '%')\n",
    "    print('% voxels for which null dist. was normal: ' + str((1 - permTest[TASK][SUB][6].iloc[0,1]) * 100) + '%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% permutation tests\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get group level null distributions for each voxel\n",
    "if numSubs > 1: # if we're running the analysis on more than one participant...\n",
    "\n",
    "    # preallocate lists for each task\n",
    "    groupNull = [[]] * 2\n",
    "    groupFitData = [[]] * 2\n",
    "\n",
    "    # for each task...\n",
    "    for TASK in [0,1]:\n",
    "\n",
    "        #feedback\n",
    "        print('starting group level null distribution fits for the ' + taskNames[TASK] + ' task')\n",
    "\n",
    "        # preallocate sublists for groupNull array\n",
    "        groupNull[TASK] = [[]] * 5\n",
    "\n",
    "        # concatenate data from the first two participants to get things started\n",
    "        groupNull[TASK][0] = np.concatenate((nullCorr[TASK][0],nullCorr[TASK][1]),axis=0)\n",
    "\n",
    "        # concatenate data from any remaining participants\n",
    "        if numSubs > 2: # if we're running the analysis on more than two participants...\n",
    "            for SUB in range(2,numSubs): # for each participant...\n",
    "                 groupNull[TASK][0] = np.concatenate((groupNull[TASK][0],nullCorr[TASK][SUB]),axis=0) # concatenate\n",
    "\n",
    "        # fit normal distribution to group null\n",
    "        groupNull[TASK][1] = [[]] * groupNull[TASK][0].shape[1] # preallocate list for each voxel\n",
    "        groupNull[TASK][2] = [[]] * groupNull[TASK][0].shape[1] # preallocate list for each voxel\n",
    "        groupNull[TASK][3] = np.zeros((groupNull[TASK][0].shape[1],), dtype=int) # preallocate vector of zeros (length = voxels) for flagging voxels with bad fits\n",
    "        for VOX in range(groupNull[TASK][0].shape[1]): # for each voxel... (i.e., each column in groupNull[TASK][0])\n",
    "            groupNull[TASK][1][VOX] = dist.fit(groupNull[TASK][0][:,VOX]) # fit normal distribution\n",
    "            groupNull[TASK][2][VOX] = stats.kstest(groupNull[TASK][0][:,VOX], \"norm\", groupNull[TASK][1][VOX]) # get KS goodness of fit\n",
    "            if groupNull[TASK][2][VOX][1] < alpha:\n",
    "                groupNull[TASK][3][VOX] = 1\n",
    "        groupNull[TASK][4] = pd.DataFrame(data={'numBadFits': [sum(groupNull[TASK][3])], 'propBadFits': [sum(groupNull[TASK][3]) / len(groupNull[TASK][3])]}) # number and proportion of voxels with bad normal fits to the group null distribution\n",
    "\n",
    "        # make another variable with just the group level fit parameters, goodness of fit results, and summary measures\n",
    "        groupFitData[TASK] = [[]] * 3\n",
    "        groupFitData[TASK] = [[]] * 3\n",
    "        groupFitData[TASK][0] = groupNull[TASK][1] # fit parameters\n",
    "        groupFitData[TASK][1] = groupNull[TASK][2][VOX] # KS test results\n",
    "        groupFitData[TASK][2] = groupNull[TASK][4]\n",
    "\n",
    "        #feedback\n",
    "        print('*** group level null fits ***')\n",
    "        print(groupFitData[TASK][2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% group level null distribution\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "endTime = time.time()\n",
    "duration = (endTime - startTime) / 60 #[min]\n",
    "print('control tasks ISC duration: ' + str(duration))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Get total analysis duration\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% try plotting on a brain\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if saveOutput:\n",
    "    import pickle\n",
    "    if circShift:\n",
    "        saveFile = folder + 'controlISC_cShift_' + saveTag + str(permutations) + 'perm.pkl'\n",
    "        permTest_c = permTest\n",
    "        corrData_c = corrData\n",
    "        groupFitData_c = groupFitData\n",
    "        duration_c = duration\n",
    "        with open(saveFile,'wb') as f:  # Python 3: open(..., 'wb')\n",
    "            pickle.dump([permTest_c, corrData_c, groupFitData_c, duration_c], f, protocol=4)\n",
    "    else:\n",
    "        saveFile = folder + 'controlISC_pScram_' + saveTag + str(permutations) + 'perm.pkl'\n",
    "        with open(saveFile,'wb') as f:  # Python 3: open(..., 'wb')\n",
    "            pickle.dump([permTest, corrData, groupFitData, duration], f, protocol=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% save data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# what load syntax should look like:\n",
    "# folder = '/afs/dbic.dartmouth.edu/usr/wheatley/jd/control_tasks/'\n",
    "# loadFile = folder + 'controlISC_cShift_200perm.pkl'\n",
    "# import pickle\n",
    "# with open(loadFile, 'rb') as f:\n",
    "#     permTest, corrData, groupFitData, duration = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for TASK in [0,1]:\n",
    "#     print('********** ' + taskNames[TASK] + 'task **********')\n",
    "#     for SUB in range(len(permTest[TASK])):\n",
    "#          print(permTest[TASK][SUB][2])\n",
    "\n",
    "print(groupFitData[0][2])\n",
    "print(groupFitData[1][2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if examplePlots:\n",
    "\n",
    "    TASK = 0 # arbitrary\n",
    "    SUB = 1 # arbitrary\n",
    "    numVox = len(permTest[TASK][SUB][4]) # get number of voxels\n",
    "\n",
    "    pVals = permTest[TASK][SUB][0]\n",
    "    min_value = min(pVals)\n",
    "    min_index = pVals.index(min_value)\n",
    "    print(min_index)\n",
    "\n",
    "    # convert KS test statistic values to numpy array\n",
    "    KSval = np.empty([numVox, 1])\n",
    "    for VOX in range(len(permTest[TASK][SUB][4])):\n",
    "        KSval[VOX] = permTest[TASK][SUB][4][VOX][0]\n",
    "\n",
    "    # get min test statistic (should be good fit)\n",
    "    min_i = np.unravel_index(np.argmin(KSval, axis=None), KSval.shape)[0]\n",
    "    print(min_i)\n",
    "\n",
    "    # get max test statistic (should be bad fit)\n",
    "    max_i = np.unravel_index(np.argmax(KSval, axis=None), KSval.shape)[0]\n",
    "    print(max_i)\n",
    "\n",
    "    # plot good fit\n",
    "    for QUAL in [min_i, max_i, 4210]:\n",
    "        VOX = QUAL\n",
    "        mu = permTest[TASK][SUB][3][VOX][0]\n",
    "        std = permTest[TASK][SUB][3][VOX][1]\n",
    "        xmin = mu - 4*std\n",
    "        xmax = mu + 4*std\n",
    "        x = np.linspace(xmin, xmax, 100)\n",
    "        p = stats.norm.pdf(x, mu, std)\n",
    "\n",
    "        plt.figure(facecolor='white')\n",
    "\n",
    "        # Generate some data for this demonstration.\n",
    "        data = nullCorr[TASK][SUB][:,VOX]\n",
    "\n",
    "        # Fit a normal distribution to the data:\n",
    "        mu, std = norm.fit(data)\n",
    "\n",
    "        # Plot the histogram.\n",
    "        plt.hist(data, bins=25, density=True, alpha=0.6, color='m')\n",
    "\n",
    "        # Plot the PDF.\n",
    "        xmin, xmax = plt.xlim()\n",
    "        x = np.linspace(xmin, xmax, 100)\n",
    "        p = norm.pdf(x, mu, std)\n",
    "        plt.plot(x, p, 'k', linewidth=2)\n",
    "        ksStat = round(permTest[TASK][SUB][4][VOX][0] * 100) / 100\n",
    "        ksPval = round(permTest[TASK][SUB][4][VOX][1] * 100) / 100\n",
    "        title = \"KS test stat = \" + str(ksStat) + \", KS pVal = \" + str(ksPval) + \", perm pVal = \" + str(pVals[VOX])\n",
    "        plt.title(title)\n",
    "\n",
    "        x2 = corrData[TASK][SUB][VOX]\n",
    "        yMax = plt.gca().get_ylim()[1]\n",
    "        plt.plot([x2, x2], [0, yMax], 'y', linewidth=3)\n",
    "\n",
    "        plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}